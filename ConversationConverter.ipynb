{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "451d46c6",
   "metadata": {},
   "source": [
    "# Acme Conversation Converter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9841206b",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "868cb2fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2c9c26eb530>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Any, List\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BartForConditionalGeneration, get_linear_schedule_with_warmup\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True      # safe, faster matmul\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4872320",
   "metadata": {},
   "source": [
    "### Load and Explore Samsum Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9e32fb09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14731 818 819\n",
      "Train shape: (14731, 3) | Val shape: (818, 3)\n",
      "Sample rows:\n",
      "          id                                           dialogue  \\\n",
      "0  13818513  Amanda: I baked  cookies. Do you want some?\\nJ...   \n",
      "1  13728867  Olivia: Who are you voting for in this electio...   \n",
      "2  13681000  Tim: Hi, what's up?\\nKim: Bad mood tbh, I was ...   \n",
      "3  13730747  Edward: Rachel, I think I'm in ove with Bella....   \n",
      "4  13728094  Sam: hey  overheard rick say something\\nSam: i...   \n",
      "\n",
      "                                             summary  \n",
      "0  Amanda baked cookies and will bring Jerry some...  \n",
      "1  Olivia and Olivier are voting for liberals in ...  \n",
      "2  Kim may try the pomodoro technique recommended...  \n",
      "3  Edward thinks he is in love with Bella. Rachel...  \n",
      "4  Sam is confused, because he overheard Rick com...  \n"
     ]
    }
   ],
   "source": [
    "# Load Samsum dataset\n",
    "df = None\n",
    "for repo in [\"samsum\", \"knkarthick/samsum\"]:\n",
    "    try:\n",
    "        df = load_dataset(repo)\n",
    "        break\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "df_train = df[\"train\"].to_pandas()[[\"id\",\"dialogue\",\"summary\"]]\n",
    "df_val   = df[\"validation\"].to_pandas()[[\"id\",\"dialogue\",\"summary\"]]\n",
    "df_test  = df[\"test\"].to_pandas()[[\"id\",\"dialogue\",\"summary\"]] if \"test\" in df else None\n",
    "\n",
    "print(len(df_train), len(df_val), len(df_test) if df_test is not None else 0)\n",
    "\n",
    "print(\"Train shape:\", df_train.shape, \"| Val shape:\", df_val.shape)\n",
    "print(\"Sample rows:\\n\", df_train.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "22a6d588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Train word count summary ===\n",
      "       dialogue_word_count  summary_word_count\n",
      "count         14731.000000        14731.000000\n",
      "mean             93.792750           20.318444\n",
      "std              74.031937           11.153570\n",
      "min               7.000000            1.000000\n",
      "25%              39.000000           12.000000\n",
      "50%              73.000000           18.000000\n",
      "75%             128.000000           27.000000\n",
      "max             803.000000           64.000000\n",
      "\n",
      "=== Validation word count summary ===\n",
      "       dialogue_word_count  summary_word_count\n",
      "count           818.000000          818.000000\n",
      "mean             91.641809           20.283619\n",
      "std              74.479672           11.211454\n",
      "min              10.000000            3.000000\n",
      "25%              38.000000           12.000000\n",
      "50%              70.000000           18.000000\n",
      "75%             127.000000           26.000000\n",
      "max             540.000000           59.000000\n"
     ]
    }
   ],
   "source": [
    "def add_len_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[\"dialogue_word_count\"] = df[\"dialogue\"].str.split().apply(len)\n",
    "    df[\"summary_word_count\"]  = df[\"summary\"].str.split().apply(len)\n",
    "    return df\n",
    "\n",
    "eda_train = add_len_cols(df_train)\n",
    "eda_val   = add_len_cols(df_val)\n",
    "\n",
    "print(\"\\n=== Train word count summary ===\")\n",
    "print(eda_train[[\"dialogue_word_count\", \"summary_word_count\"]].describe())\n",
    "\n",
    "print(\"\\n=== Validation word count summary ===\")\n",
    "print(eda_val[[\"dialogue_word_count\", \"summary_word_count\"]].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4434951",
   "metadata": {},
   "source": [
    "### BART Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dba3b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batches/epoch: 1842 val: 103\n"
     ]
    }
   ],
   "source": [
    "# TOKENIZERS (BERT + GPT)\n",
    "# BART tokenizer & model\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "bart_name = \"sshleifer/distilbart-cnn-12-6\"\n",
    "bart_tok = AutoTokenizer.from_pretrained(bart_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(bart_name).to(device)\n",
    "\n",
    "model.gradient_checkpointing_disable()   # saves memory\n",
    "model.config.use_cache = False\n",
    "\n",
    "model.config.output_attentions = False\n",
    "model.config.output_hidden_states = False\n",
    "\n",
    "\n",
    "\n",
    "class BartSumDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_source_len=512, max_target_len=128):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tok = tokenizer\n",
    "        self.src_len = max_source_len\n",
    "        self.tgt_len = max_target_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        row = self.df.iloc[idx]\n",
    "        dialogue = str(row[\"dialogue\"])\n",
    "        summary  = str(row[\"summary\"])\n",
    "\n",
    "        # Encode source (dialogue)\n",
    "        enc = self.tok(\n",
    "            dialogue,\n",
    "            max_length=self.src_len,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",          # returns torch.Tensor\n",
    "        )\n",
    "\n",
    "        # Encode target (summary)\n",
    "        dec = self.tok(\n",
    "            text_target=summary,\n",
    "            max_length=self.tgt_len,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",          # returns torch.Tensor\n",
    "        )\n",
    "\n",
    "        labels = dec[\"input_ids\"].clone()\n",
    "        labels[labels == self.tok.pad_token_id] = -100\n",
    "\n",
    "        # Squeeze away the batch dim since return_tensors=\"pt\" adds it\n",
    "        return {\n",
    "            \"input_ids\": enc[\"input_ids\"].squeeze(0),         # torch.LongTensor\n",
    "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": labels.squeeze(0),\n",
    "        }\n",
    "\n",
    "# lengths & batch (full training settings)\n",
    "MAX_SOURCE_LEN  = 320\n",
    "MAX_TARGET_LEN = 96\n",
    "BATCH_SIZE   = 8\n",
    "NUM_WORKERS  = 0\n",
    "PIN_MEMORY   = torch.cuda.is_available()\n",
    "PREFETCH     = None\n",
    "\n",
    "# For TRAINING speed:\n",
    "model.config.use_cache = False        # cache hurts training speed/mem\n",
    "model.gradient_checkpointing_enable()# disable for speed (enable only if you need memory)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "\n",
    "train_ds = BartSumDataset(df_train, bart_tok, MAX_SOURCE_LEN, MAX_TARGET_LEN)\n",
    "val_ds   = BartSumDataset(df_val,   bart_tok, MAX_SOURCE_LEN, MAX_TARGET_LEN)\n",
    "test_ds  = BartSumDataset(df_test,  bart_tok, MAX_SOURCE_LEN, MAX_TARGET_LEN) if df_test is not None else None\n",
    "\n",
    "pad_to_mult = 8 if torch.cuda.is_available() else None\n",
    "\n",
    "collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=bart_tok,\n",
    "    model=model,                     \n",
    "    label_pad_token_id=-100,\n",
    "    pad_to_multiple_of=pad_to_mult\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          collate_fn=collator, num_workers=NUM_WORKERS,\n",
    "                          pin_memory=PIN_MEMORY, prefetch_factor=PREFETCH, persistent_workers=False)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          collate_fn=collator, num_workers=NUM_WORKERS,\n",
    "                          pin_memory=PIN_MEMORY, prefetch_factor=PREFETCH, persistent_workers=False)\n",
    "test_loader  = (DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                           collate_fn=collator, num_workers=NUM_WORKERS, \n",
    "                           pin_memory=PIN_MEMORY, prefetch_factor=PREFETCH, persistent_workers=False)\n",
    "                if test_ds is not None else None)\n",
    "print(\"Batches/epoch:\", len(train_loader), \"val:\", len(val_loader))\n",
    "\n",
    "# Prompt template for causal LM\n",
    "PROMPT_PREFIX = \"Summarize the following dialogue:\\n\"\n",
    "PROMPT_SUFFIX = \"\\nSummary:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "68c7ebbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = BartForConditionalGeneration.from_pretrained(bart_name).to(device)\n",
    "model.config.pad_token_id = bart_tok.pad_token_id\n",
    "model.config.bos_token_id = bart_tok.bos_token_id\n",
    "model.config.eos_token_id = bart_tok.eos_token_id\n",
    "\n",
    "# Epochs / lr / regularization\n",
    "EPOCHS            = 3\n",
    "LR                = 3e-5\n",
    "WEIGHT_DECAY      = 0.01\n",
    "GRAD_ACCUM_STEPS  = 4      # effective batch ~= BATCH_SIZE * GRAD_ACCUM_STEPS\n",
    "MAX_GRAD_NORM     = 1.0\n",
    "WARMUP_RATIO      = 0.06\n",
    "\n",
    "# Optimizer (AdamW) with weight decay on non-bias/LayerNorm\n",
    "decay, no_decay = [], []\n",
    "for n,p in model.named_parameters():\n",
    "    (decay if not any(nd in n for nd in [\"bias\",\"LayerNorm.weight\"]) else no_decay).append(p)\n",
    "optimizer = AdamW(\n",
    "    [{\"params\": decay, \"weight_decay\": WEIGHT_DECAY},\n",
    "     {\"params\": no_decay, \"weight_decay\": 0.0}],\n",
    "    lr=LR,\n",
    "    fused=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "\n",
    "updates_per_epoch = (len(train_loader) + GRAD_ACCUM_STEPS - 1) // GRAD_ACCUM_STEPS\n",
    "total_updates     = EPOCHS * updates_per_epoch\n",
    "warmup_steps      = max(1, int(WARMUP_RATIO * total_updates))\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, warmup_steps, total_updates)\n",
    "\n",
    "scaler = torch.amp.GradScaler(\"cuda\", enabled=(torch.cuda.is_available()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6246ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417c1a7f",
   "metadata": {},
   "source": [
    "### Training BART Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "706bb4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E1] upd 10/1383 | loss 2.9553\n",
      "[E1] upd 20/1383 | loss 2.2832\n",
      "[E1] upd 30/1383 | loss 1.9457\n",
      "[E1] upd 40/1383 | loss 1.8372\n",
      "[E1] upd 50/1383 | loss 1.8606\n",
      "[E1] upd 60/1383 | loss 1.7278\n",
      "[E1] upd 70/1383 | loss 1.7562\n",
      "[E1] upd 80/1383 | loss 1.7223\n",
      "[E1] upd 90/1383 | loss 1.6419\n",
      "[E1] upd 100/1383 | loss 1.7306\n",
      "[E1] upd 110/1383 | loss 1.6942\n",
      "[E1] upd 120/1383 | loss 1.6607\n",
      "[E1] upd 130/1383 | loss 1.6507\n",
      "[E1] upd 140/1383 | loss 1.7588\n",
      "[E1] upd 150/1383 | loss 1.6814\n",
      "[E1] upd 160/1383 | loss 1.6132\n",
      "[E1] upd 170/1383 | loss 1.6255\n",
      "[E1] upd 180/1383 | loss 1.6999\n",
      "[E1] upd 190/1383 | loss 1.6131\n",
      "[E1] upd 200/1383 | loss 1.6252\n",
      "[E1] upd 210/1383 | loss 1.6923\n",
      "[E1] upd 220/1383 | loss 1.5971\n",
      "[E1] upd 230/1383 | loss 1.5886\n",
      "[E1] upd 240/1383 | loss 1.6558\n",
      "[E1] upd 250/1383 | loss 1.6051\n",
      "[E1] upd 260/1383 | loss 1.6142\n",
      "[E1] upd 270/1383 | loss 1.6184\n",
      "[E1] upd 280/1383 | loss 1.6106\n",
      "[E1] upd 290/1383 | loss 1.5640\n",
      "[E1] upd 300/1383 | loss 1.5570\n",
      "[E1] upd 310/1383 | loss 1.5437\n",
      "[E1] upd 320/1383 | loss 1.5237\n",
      "[E1] upd 330/1383 | loss 1.5721\n",
      "[E1] upd 340/1383 | loss 1.6548\n",
      "[E1] upd 350/1383 | loss 1.5060\n",
      "[E1] upd 360/1383 | loss 1.5984\n",
      "[E1] upd 370/1383 | loss 1.5835\n",
      "[E1] upd 380/1383 | loss 1.5573\n",
      "[E1] upd 390/1383 | loss 1.5396\n",
      "[E1] upd 400/1383 | loss 1.5332\n",
      "[E1] upd 410/1383 | loss 1.5857\n",
      "[E1] upd 420/1383 | loss 1.5560\n",
      "[E1] upd 430/1383 | loss 1.5019\n",
      "[E1] upd 440/1383 | loss 1.5610\n",
      "[E1] upd 450/1383 | loss 1.5380\n",
      "[E1] upd 460/1383 | loss 1.5621\n",
      "[E2] upd 470/1383 | loss 1.3216\n",
      "[E2] upd 480/1383 | loss 1.3359\n",
      "[E2] upd 490/1383 | loss 1.3020\n",
      "[E2] upd 500/1383 | loss 1.3251\n",
      "[E2] upd 510/1383 | loss 1.3567\n",
      "[E2] upd 520/1383 | loss 1.3005\n",
      "[E2] upd 530/1383 | loss 1.3374\n",
      "[E2] upd 540/1383 | loss 1.3399\n",
      "[E2] upd 550/1383 | loss 1.3283\n",
      "[E2] upd 560/1383 | loss 1.3272\n",
      "[E2] upd 570/1383 | loss 1.3098\n",
      "[E2] upd 580/1383 | loss 1.3630\n",
      "[E2] upd 590/1383 | loss 1.3248\n",
      "[E2] upd 600/1383 | loss 1.2448\n",
      "[E2] upd 610/1383 | loss 1.3266\n",
      "[E2] upd 620/1383 | loss 1.3649\n",
      "[E2] upd 630/1383 | loss 1.3285\n",
      "[E2] upd 640/1383 | loss 1.3637\n",
      "[E2] upd 650/1383 | loss 1.3010\n",
      "[E2] upd 660/1383 | loss 1.2644\n",
      "[E2] upd 670/1383 | loss 1.3581\n",
      "[E2] upd 680/1383 | loss 1.2755\n",
      "[E2] upd 690/1383 | loss 1.2469\n",
      "[E2] upd 700/1383 | loss 1.3263\n",
      "[E2] upd 710/1383 | loss 1.3138\n",
      "[E2] upd 720/1383 | loss 1.3001\n",
      "[E2] upd 730/1383 | loss 1.2987\n",
      "[E2] upd 740/1383 | loss 1.3011\n",
      "[E2] upd 750/1383 | loss 1.2797\n",
      "[E2] upd 760/1383 | loss 1.3550\n",
      "[E2] upd 770/1383 | loss 1.2756\n",
      "[E2] upd 780/1383 | loss 1.1790\n",
      "[E2] upd 790/1383 | loss 1.2886\n",
      "[E2] upd 800/1383 | loss 1.3047\n",
      "[E2] upd 810/1383 | loss 1.2723\n",
      "[E2] upd 820/1383 | loss 1.2686\n",
      "[E2] upd 830/1383 | loss 1.3388\n",
      "[E2] upd 840/1383 | loss 1.2881\n",
      "[E2] upd 850/1383 | loss 1.3114\n",
      "[E2] upd 860/1383 | loss 1.3072\n",
      "[E2] upd 870/1383 | loss 1.3183\n",
      "[E2] upd 880/1383 | loss 1.3061\n",
      "[E2] upd 890/1383 | loss 1.3068\n",
      "[E2] upd 900/1383 | loss 1.3288\n",
      "[E2] upd 910/1383 | loss 1.2556\n",
      "[E2] upd 920/1383 | loss 1.3082\n",
      "[E3] upd 930/1383 | loss 1.1540\n",
      "[E3] upd 940/1383 | loss 1.1718\n",
      "[E3] upd 950/1383 | loss 1.1236\n",
      "[E3] upd 960/1383 | loss 1.1060\n",
      "[E3] upd 970/1383 | loss 1.1336\n",
      "[E3] upd 980/1383 | loss 1.1448\n",
      "[E3] upd 990/1383 | loss 1.1196\n",
      "[E3] upd 1000/1383 | loss 1.1308\n",
      "[E3] upd 1010/1383 | loss 1.1368\n",
      "[E3] upd 1020/1383 | loss 1.1514\n",
      "[E3] upd 1030/1383 | loss 1.1342\n",
      "[E3] upd 1040/1383 | loss 1.1145\n",
      "[E3] upd 1050/1383 | loss 1.1128\n",
      "[E3] upd 1060/1383 | loss 1.1258\n",
      "[E3] upd 1070/1383 | loss 1.0818\n",
      "[E3] upd 1080/1383 | loss 1.0854\n",
      "[E3] upd 1090/1383 | loss 1.1216\n",
      "[E3] upd 1100/1383 | loss 1.1510\n",
      "[E3] upd 1110/1383 | loss 1.1445\n",
      "[E3] upd 1120/1383 | loss 1.0862\n",
      "[E3] upd 1130/1383 | loss 1.1057\n",
      "[E3] upd 1140/1383 | loss 1.1008\n",
      "[E3] upd 1150/1383 | loss 1.1104\n",
      "[E3] upd 1160/1383 | loss 1.1092\n",
      "[E3] upd 1170/1383 | loss 1.1017\n",
      "[E3] upd 1180/1383 | loss 1.1784\n",
      "[E3] upd 1190/1383 | loss 1.1642\n",
      "[E3] upd 1200/1383 | loss 1.1285\n",
      "[E3] upd 1210/1383 | loss 1.1538\n",
      "[E3] upd 1220/1383 | loss 1.0833\n",
      "[E3] upd 1230/1383 | loss 1.1177\n",
      "[E3] upd 1240/1383 | loss 1.1011\n",
      "[E3] upd 1250/1383 | loss 1.1121\n",
      "[E3] upd 1260/1383 | loss 1.1403\n",
      "[E3] upd 1270/1383 | loss 1.1714\n",
      "[E3] upd 1280/1383 | loss 1.0913\n",
      "[E3] upd 1290/1383 | loss 1.0854\n",
      "[E3] upd 1300/1383 | loss 1.1492\n",
      "[E3] upd 1310/1383 | loss 1.1804\n",
      "[E3] upd 1320/1383 | loss 1.1125\n",
      "[E3] upd 1330/1383 | loss 1.1243\n",
      "[E3] upd 1340/1383 | loss 1.1195\n",
      "[E3] upd 1350/1383 | loss 1.1382\n",
      "[E3] upd 1360/1383 | loss 1.0907\n",
      "[E3] upd 1370/1383 | loss 1.1015\n",
      "[E3] upd 1380/1383 | loss 1.1030\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    device = next(model.parameters()).device\n",
    "    model.train()\n",
    "    global_step = 0\n",
    "    log_every_updates = 10     # prints every N optimizer updates (not batches)\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        running_loss = 0.0\n",
    "        running_updates = 0\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # OPTIONAL: warm up one batch to trigger kernel JIT etc.\n",
    "        # (Helps make your first timings realistic)\n",
    "        # _warm = next(iter(train_loader))\n",
    "        # _warm = {k: v.to(device, non_blocking=True) for k,v in _warm.items()}\n",
    "        # with torch.cuda.amp.autocast(enabled=(device.type==\"cuda\")):\n",
    "        #     _ = model(**_warm).loss.backward()\n",
    "        # optimizer.zero_grad(set_to_none=True)\n",
    "        # if device.type == \"cuda\": torch.cuda.synchronize()\n",
    "\n",
    "        for step, batch in enumerate(train_loader, 1):\n",
    "            batch = {k: v.to(device, non_blocking=True) for k, v in batch.items()}\n",
    "\n",
    "            with torch.amp.autocast(\"cuda\", enabled=(device.type==\"cuda\")):\n",
    "                out  = model(**batch)                         # Seq2SeqLM returns .loss\n",
    "                loss = out.loss / GRAD_ACCUM_STEPS\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if step % GRAD_ACCUM_STEPS == 0:\n",
    "                # Unscale before clipping for stable norms\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
    "\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                scheduler.step()\n",
    "\n",
    "                global_step += 1\n",
    "                running_updates += 1\n",
    "\n",
    "                if running_updates % log_every_updates == 0:\n",
    "                    # report avg loss over the last `log_every_updates` updates\n",
    "                    avg = running_loss / (log_every_updates)\n",
    "                    print(f\"[E{epoch}] upd {global_step}/{total_updates} | loss {avg:.4f}\")\n",
    "                    running_loss = 0.0\n",
    "\n",
    "        # end epoch\n",
    "        if running_updates % log_every_updates != 0 and running_updates > 0:\n",
    "            avg = running_loss / max(1, (running_updates % log_every_updates))\n",
    "            print(f\"[E{epoch}] upd {global_step}/{total_updates} | loss {avg:.4f}\")\n",
    "\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd02366",
   "metadata": {},
   "source": [
    "### Rouge-lite function for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5fd10e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "_word_re = re.compile(r\"\\w+('\\w+)?\", re.UNICODE)\n",
    "\n",
    "def _tokenize(s: str):\n",
    "    return _word_re.findall(s.lower())\n",
    "\n",
    "def _f1(overlap, ref_count, pred_count):\n",
    "    if overlap == 0: return 0.0\n",
    "    precision = overlap / pred_count if pred_count else 0.0\n",
    "    recall    = overlap / ref_count  if ref_count  else 0.0\n",
    "    if precision + recall == 0: return 0.0\n",
    "    return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "def _ngram_counts(tokens, n):\n",
    "    return Counter(tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1))\n",
    "\n",
    "def _rouge_n(ref_tokens, pred_tokens, n):\n",
    "    ref = _ngram_counts(ref_tokens, n)\n",
    "    pred = _ngram_counts(pred_tokens, n)\n",
    "    overlap = sum((ref & pred).values())\n",
    "    return _f1(overlap, sum(ref.values()), sum(pred.values()))\n",
    "\n",
    "def _lcs_len(a, b):\n",
    "    # classic DP LCS length\n",
    "    m, n = len(a), len(b)\n",
    "    dp = [0]*(n+1)\n",
    "    for i in range(1, m+1):\n",
    "        prev = 0\n",
    "        for j in range(1, n+1):\n",
    "            tmp = dp[j]\n",
    "            if a[i-1] == b[j-1]:\n",
    "                dp[j] = prev + 1\n",
    "            else:\n",
    "                dp[j] = max(dp[j], dp[j-1])\n",
    "            prev = tmp\n",
    "    return dp[-1]\n",
    "\n",
    "def _rouge_l(ref_tokens, pred_tokens):\n",
    "    lcs = _lcs_len(ref_tokens, pred_tokens)\n",
    "    return _f1(lcs, len(ref_tokens), len(pred_tokens))\n",
    "\n",
    "def compute_rouge_light(preds, refs):\n",
    "    \"\"\"NLTK-free ROUGE: returns {'rouge1','rouge2','rougeL'} (F1).\"\"\"\n",
    "    assert len(preds) == len(refs)\n",
    "    r1=r2=rl=0.0\n",
    "    for p, r in zip(preds, refs):\n",
    "        pt = _tokenize(p); rt = _tokenize(r)\n",
    "        r1 += _rouge_n(rt, pt, 1)\n",
    "        r2 += _rouge_n(rt, pt, 2)\n",
    "        rl += _rouge_l(rt, pt)\n",
    "    n = max(1, len(preds))\n",
    "    return {\"rouge1\": r1/n, \"rouge2\": r2/n, \"rougeL\": rl/n}\n",
    "\n",
    "bertscore_metric = evaluate.load(\"bertscore\")\n",
    "\n",
    "def get_bertscore(preds, refs, lang=\"en\"):\n",
    "    \"\"\"\n",
    "    Compute BERTScore F1 between predictions and references.\n",
    "    Args:\n",
    "        preds (list[str]): Model-generated summaries\n",
    "        refs (list[str]): Reference summaries\n",
    "        lang (str): Language code (default: English)\n",
    "    Returns:\n",
    "        float: Average F1 score\n",
    "    \"\"\"\n",
    "    results = bertscore_metric.compute(predictions=preds, references=refs, lang=lang)\n",
    "    return float(sum(results[\"f1\"]) / len(results[\"f1\"]))\n",
    "    \n",
    "def basic_diag(preds, refs):\n",
    "    import numpy as np, re\n",
    "    def wc(s): return len(re.findall(r\"\\w+\", s))\n",
    "    len_pred = np.array([wc(p) for p in preds]); len_ref = np.array([wc(r) for r in refs])\n",
    "    ratios   = len_pred / np.maximum(1, len_ref)\n",
    "    return {\n",
    "        \"avg_len_pred\": float(len_pred.mean()),\n",
    "        \"avg_len_ref\":  float(len_ref.mean()),\n",
    "        \"len_ratio_mean\": float(ratios.mean()),\n",
    "        \"len_ratio_median\": float(np.median(ratios)),\n",
    "    }\n",
    "\n",
    "def decode_refs_from_labels(tokenizer, label_tensor_batch):\n",
    "    refs = []\n",
    "    for t in label_tensor_batch:\n",
    "        t = t[t != -100]\n",
    "        refs.append(tokenizer.decode(t, skip_special_tokens=True).strip())\n",
    "    return refs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b958611",
   "metadata": {},
   "source": [
    "### BART Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "90b561ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import torch\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_bart(model, loader, tokenizer, max_len=128, num_beams=4, limit=None):\n",
    "    model.eval()\n",
    "    preds, refs = [], []\n",
    "    for i, batch in enumerate(loader):\n",
    "        if limit is not None and i >= limit: break\n",
    "        inp = batch[\"input_ids\"].to(model.device)\n",
    "        att = batch[\"attention_mask\"].to(model.device)\n",
    "        gen = model.generate(\n",
    "            input_ids=inp, attention_mask=att,\n",
    "            max_length=max_len, num_beams=num_beams, early_stopping=True,\n",
    "            pad_token_id=tokenizer.pad_token_id, eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "        preds.extend(tokenizer.batch_decode(gen, skip_special_tokens=True))\n",
    "        refs.extend(decode_refs_from_labels(tokenizer, batch[\"labels\"]))\n",
    "    r = compute_rouge_light(preds, refs)\n",
    "    bs = get_bertscore(preds, refs) \n",
    "    diag = basic_diag(preds, refs)\n",
    "    df = pd.DataFrame({\"model\":\"BART\", \"pred\": preds, \"ref\": refs})\n",
    "    return r, bs, diag, df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc7738e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use existing loaders;\n",
    "MAX_TARGET_LEN = 96  # keep consistent with training\n",
    "\n",
    "preds, refs, scores = evaluate_bart(\n",
    "    model, val_loader, bart_tok,\n",
    "    max_len=MAX_TARGET_LEN,\n",
    "    num_beams=4,        # try 2 for speed, or 1 for greedy\n",
    "    limit= None          # set to e.g. 100 to do a quick subset\n",
    ")\n",
    "\n",
    "print({k: round(v, 4) for k, v in scores.items()})\n",
    "\n",
    "# Save side-by-side for inspection\n",
    "import pandas as pd\n",
    "pd.DataFrame({\"pred\": preds, \"ref\": refs}).to_csv(\"bart_val_predictions.csv\", index=False)\n",
    "print(\"Saved: bart_val_predictions.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6183daad",
   "metadata": {},
   "outputs": [],
   "source": [
    "bertscore = evaluate.load(\"bertscore\")\n",
    "scores = bertscore.compute(predictions=preds, references=refs, lang=\"en\")\n",
    "print(\"BERTScore F1:\", sum(scores[\"f1\"])/len(scores[\"f1\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35358c5a",
   "metadata": {},
   "source": [
    "#### GPT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "389f9fa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPT_NAME = \"distilgpt2\"\n",
    "\n",
    "gpt_tok = AutoTokenizer.from_pretrained(GPT_NAME)\n",
    "if gpt_tok.pad_token is None:  # ensure proper padding & clean decoding\n",
    "    gpt_tok.pad_token = gpt_tok.eos_token\n",
    "\n",
    "gpt = AutoModelForCausalLM.from_pretrained(GPT_NAME)\n",
    "gpt.resize_token_embeddings(len(gpt_tok))  # if pad token was added\n",
    "gpt.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2f9f6145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building GPT Dataset\n",
    "class GPTDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for auto-regressive LM summarization.\n",
    "    Builds: prompt = PREFIX + dialogue + SUFFIX + \" \" + summary\n",
    "    Masks prompt tokens with -100 so only summary contributes to loss.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, tokenizer, max_length=1024,\n",
    "                 prompt_prefix=\"Summarize the following dialogue:\\n\",\n",
    "                 prompt_suffix=\"\\nSummary:\"):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tok = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.prefix = prompt_prefix\n",
    "        self.suffix = prompt_suffix\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        dialogue = str(row[\"dialogue\"])\n",
    "        summary  = str(row[\"summary\"])\n",
    "\n",
    "        # prompt = instruction + dialogue + suffix\n",
    "        prompt = f\"{self.prefix}{dialogue}{self.suffix} \"\n",
    "        tok_prompt = self.tok(prompt, add_special_tokens=False,\n",
    "                              truncation=True, max_length=self.max_length)\n",
    "        tok_full   = self.tok(prompt + summary, add_special_tokens=False,\n",
    "                              truncation=True, max_length=self.max_length)\n",
    "\n",
    "        ids = tok_full[\"input_ids\"][: self.max_length]\n",
    "        attn = [1] * len(ids)\n",
    "\n",
    "        # mask out prompt portion\n",
    "        prompt_len = min(len(tok_prompt[\"input_ids\"]), len(ids))\n",
    "        labels = [-100] * prompt_len + ids[prompt_len:]\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(ids, dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(attn, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "21aecba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT Collator\n",
    "@dataclass\n",
    "class GPTCollator:\n",
    "    tokenizer: any\n",
    "    label_pad_id: int = -100\n",
    "\n",
    "    def __call__(self, features):\n",
    "        input_ids = [f[\"input_ids\"] for f in features]\n",
    "        attn      = [f[\"attention_mask\"] for f in features]\n",
    "        labels    = [f[\"labels\"] for f in features]\n",
    "\n",
    "        input_ids_padded = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        attn_padded = torch.nn.utils.rnn.pad_sequence(\n",
    "            attn, batch_first=True, padding_value=0\n",
    "        )\n",
    "        labels_padded = torch.nn.utils.rnn.pad_sequence(\n",
    "            labels, batch_first=True, padding_value=self.label_pad_id\n",
    "        )\n",
    "\n",
    "        return {\"input_ids\": input_ids_padded,\n",
    "                \"attention_mask\": attn_padded,\n",
    "                \"labels\": labels_padded}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d973c2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT loaders ready: 1842 52\n"
     ]
    }
   ],
   "source": [
    "MAX_GPT_LEN  = 1024\n",
    "BATCH_SIZE   = 8\n",
    "NUM_WORKERS  = 0\n",
    "PIN_MEMORY   = torch.cuda.is_available()\n",
    "PREFETCH     = None\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")  # TF32 fast matmuls on Ampere+\n",
    "torch.backends.cuda.enable_flash_sdp(True)\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(True)\n",
    "torch.backends.cuda.enable_math_sdp(True)\n",
    "\n",
    "gpt.gradient_checkpointing_disable()\n",
    "gpt.config.use_cache = False\n",
    "\n",
    "gpt_train_ds = GPTDataset(df_train, gpt_tok, max_length=MAX_GPT_LEN,\n",
    "                                prompt_prefix=PROMPT_PREFIX, prompt_suffix=PROMPT_SUFFIX)\n",
    "gpt_val_ds   = GPTDataset(df_val,   gpt_tok, max_length=MAX_GPT_LEN,\n",
    "                                prompt_prefix=PROMPT_PREFIX, prompt_suffix=PROMPT_SUFFIX)\n",
    "\n",
    "gpt_collator = GPTCollator(tokenizer=gpt_tok)\n",
    "\n",
    "gpt_train_loader = DataLoader(\n",
    "    gpt_train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    collate_fn=gpt_collator, num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY, prefetch_factor=PREFETCH, persistent_workers=False\n",
    ")\n",
    "gpt_val_loader = DataLoader(\n",
    "    gpt_val_ds, batch_size=16, shuffle=False,\n",
    "    collate_fn=gpt_collator, num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY, prefetch_factor=PREFETCH, persistent_workers=False\n",
    ")\n",
    "print(\"GPT loaders ready:\", len(gpt_train_loader), len(gpt_val_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dce5184",
   "metadata": {},
   "source": [
    "### GPT Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e472d445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GPT][E1] step 40/1842 | running_loss 0.2598\n",
      "[GPT][E1] step 80/1842 | running_loss 0.2719\n",
      "[GPT][E1] step 120/1842 | running_loss 0.2621\n",
      "[GPT][E1] step 160/1842 | running_loss 0.2669\n",
      "[GPT][E1] step 200/1842 | running_loss 0.2719\n",
      "[GPT][E1] step 240/1842 | running_loss 0.2554\n",
      "[GPT][E1] step 280/1842 | running_loss 0.2733\n",
      "[GPT][E1] step 320/1842 | running_loss 0.2604\n",
      "[GPT][E1] step 360/1842 | running_loss 0.2614\n",
      "[GPT][E1] step 400/1842 | running_loss 0.2654\n",
      "[GPT][E1] step 440/1842 | running_loss 0.2721\n",
      "[GPT][E1] step 480/1842 | running_loss 0.2661\n",
      "[GPT][E1] step 520/1842 | running_loss 0.2672\n",
      "[GPT][E1] step 560/1842 | running_loss 0.2688\n",
      "[GPT][E1] step 600/1842 | running_loss 0.2658\n",
      "[GPT][E1] step 640/1842 | running_loss 0.2600\n",
      "[GPT][E1] step 680/1842 | running_loss 0.2654\n",
      "[GPT][E1] step 720/1842 | running_loss 0.2619\n",
      "[GPT][E1] step 760/1842 | running_loss 0.2615\n",
      "[GPT][E1] step 800/1842 | running_loss 0.2616\n",
      "[GPT][E1] step 840/1842 | running_loss 0.2708\n",
      "[GPT][E1] step 880/1842 | running_loss 0.2591\n",
      "[GPT][E1] step 920/1842 | running_loss 0.2636\n",
      "[GPT][E1] step 960/1842 | running_loss 0.2631\n",
      "[GPT][E1] step 1000/1842 | running_loss 0.2662\n",
      "[GPT][E1] step 1040/1842 | running_loss 0.2636\n",
      "[GPT][E1] step 1080/1842 | running_loss 0.2568\n",
      "[GPT][E1] step 1120/1842 | running_loss 0.2630\n",
      "[GPT][E1] step 1160/1842 | running_loss 0.2683\n",
      "[GPT][E1] step 1200/1842 | running_loss 0.2708\n",
      "[GPT][E1] step 1240/1842 | running_loss 0.2552\n",
      "[GPT][E1] step 1280/1842 | running_loss 0.2662\n",
      "[GPT][E1] step 1320/1842 | running_loss 0.2654\n",
      "[GPT][E1] step 1360/1842 | running_loss 0.2627\n",
      "[GPT][E1] step 1400/1842 | running_loss 0.2662\n",
      "[GPT][E1] step 1440/1842 | running_loss 0.2587\n",
      "[GPT][E1] step 1480/1842 | running_loss 0.2605\n",
      "[GPT][E1] step 1520/1842 | running_loss 0.2678\n",
      "[GPT][E1] step 1560/1842 | running_loss 0.2669\n",
      "[GPT][E1] step 1600/1842 | running_loss 0.2689\n",
      "[GPT][E1] step 1640/1842 | running_loss 0.2598\n",
      "[GPT][E1] step 1680/1842 | running_loss 0.2630\n",
      "[GPT][E1] step 1720/1842 | running_loss 0.2665\n",
      "[GPT][E1] step 1760/1842 | running_loss 0.2605\n",
      "[GPT][E1] step 1800/1842 | running_loss 0.2523\n",
      "[GPT][E1] step 1840/1842 | running_loss 0.2612\n",
      "[GPT][E2] step 40/1842 | running_loss 0.2581\n",
      "[GPT][E2] step 80/1842 | running_loss 0.2597\n",
      "[GPT][E2] step 120/1842 | running_loss 0.2480\n",
      "[GPT][E2] step 160/1842 | running_loss 0.2569\n",
      "[GPT][E2] step 200/1842 | running_loss 0.2445\n",
      "[GPT][E2] step 240/1842 | running_loss 0.2493\n",
      "[GPT][E2] step 280/1842 | running_loss 0.2519\n",
      "[GPT][E2] step 320/1842 | running_loss 0.2474\n",
      "[GPT][E2] step 360/1842 | running_loss 0.2511\n",
      "[GPT][E2] step 400/1842 | running_loss 0.2537\n",
      "[GPT][E2] step 440/1842 | running_loss 0.2541\n",
      "[GPT][E2] step 480/1842 | running_loss 0.2541\n",
      "[GPT][E2] step 520/1842 | running_loss 0.2467\n",
      "[GPT][E2] step 560/1842 | running_loss 0.2588\n",
      "[GPT][E2] step 600/1842 | running_loss 0.2532\n",
      "[GPT][E2] step 640/1842 | running_loss 0.2492\n",
      "[GPT][E2] step 680/1842 | running_loss 0.2536\n",
      "[GPT][E2] step 720/1842 | running_loss 0.2437\n",
      "[GPT][E2] step 760/1842 | running_loss 0.2479\n",
      "[GPT][E2] step 800/1842 | running_loss 0.2461\n",
      "[GPT][E2] step 840/1842 | running_loss 0.2518\n",
      "[GPT][E2] step 880/1842 | running_loss 0.2512\n",
      "[GPT][E2] step 920/1842 | running_loss 0.2432\n",
      "[GPT][E2] step 960/1842 | running_loss 0.2534\n",
      "[GPT][E2] step 1000/1842 | running_loss 0.2598\n",
      "[GPT][E2] step 1040/1842 | running_loss 0.2539\n",
      "[GPT][E2] step 1080/1842 | running_loss 0.2404\n",
      "[GPT][E2] step 1120/1842 | running_loss 0.2492\n",
      "[GPT][E2] step 1160/1842 | running_loss 0.2503\n",
      "[GPT][E2] step 1200/1842 | running_loss 0.2469\n",
      "[GPT][E2] step 1240/1842 | running_loss 0.2487\n",
      "[GPT][E2] step 1280/1842 | running_loss 0.2507\n",
      "[GPT][E2] step 1320/1842 | running_loss 0.2487\n",
      "[GPT][E2] step 1360/1842 | running_loss 0.2506\n",
      "[GPT][E2] step 1400/1842 | running_loss 0.2419\n",
      "[GPT][E2] step 1440/1842 | running_loss 0.2501\n",
      "[GPT][E2] step 1480/1842 | running_loss 0.2509\n",
      "[GPT][E2] step 1520/1842 | running_loss 0.2456\n",
      "[GPT][E2] step 1560/1842 | running_loss 0.2509\n",
      "[GPT][E2] step 1600/1842 | running_loss 0.2412\n",
      "[GPT][E2] step 1640/1842 | running_loss 0.2456\n",
      "[GPT][E2] step 1680/1842 | running_loss 0.2412\n",
      "[GPT][E2] step 1720/1842 | running_loss 0.2459\n",
      "[GPT][E2] step 1760/1842 | running_loss 0.2475\n",
      "[GPT][E2] step 1800/1842 | running_loss 0.2495\n",
      "[GPT][E2] step 1840/1842 | running_loss 0.2530\n",
      "[GPT][E3] step 40/1842 | running_loss 0.2462\n",
      "[GPT][E3] step 80/1842 | running_loss 0.2428\n",
      "[GPT][E3] step 120/1842 | running_loss 0.2448\n",
      "[GPT][E3] step 160/1842 | running_loss 0.2415\n",
      "[GPT][E3] step 200/1842 | running_loss 0.2453\n",
      "[GPT][E3] step 240/1842 | running_loss 0.2378\n",
      "[GPT][E3] step 280/1842 | running_loss 0.2398\n",
      "[GPT][E3] step 320/1842 | running_loss 0.2457\n",
      "[GPT][E3] step 360/1842 | running_loss 0.2390\n",
      "[GPT][E3] step 400/1842 | running_loss 0.2382\n",
      "[GPT][E3] step 440/1842 | running_loss 0.2524\n",
      "[GPT][E3] step 480/1842 | running_loss 0.2426\n",
      "[GPT][E3] step 520/1842 | running_loss 0.2397\n",
      "[GPT][E3] step 560/1842 | running_loss 0.2333\n",
      "[GPT][E3] step 600/1842 | running_loss 0.2417\n",
      "[GPT][E3] step 640/1842 | running_loss 0.2448\n",
      "[GPT][E3] step 680/1842 | running_loss 0.2383\n",
      "[GPT][E3] step 720/1842 | running_loss 0.2394\n",
      "[GPT][E3] step 760/1842 | running_loss 0.2441\n",
      "[GPT][E3] step 800/1842 | running_loss 0.2507\n",
      "[GPT][E3] step 840/1842 | running_loss 0.2300\n",
      "[GPT][E3] step 880/1842 | running_loss 0.2457\n",
      "[GPT][E3] step 920/1842 | running_loss 0.2341\n",
      "[GPT][E3] step 960/1842 | running_loss 0.2368\n",
      "[GPT][E3] step 1000/1842 | running_loss 0.2332\n",
      "[GPT][E3] step 1040/1842 | running_loss 0.2463\n",
      "[GPT][E3] step 1080/1842 | running_loss 0.2408\n",
      "[GPT][E3] step 1120/1842 | running_loss 0.2410\n",
      "[GPT][E3] step 1160/1842 | running_loss 0.2433\n",
      "[GPT][E3] step 1200/1842 | running_loss 0.2398\n",
      "[GPT][E3] step 1240/1842 | running_loss 0.2440\n",
      "[GPT][E3] step 1280/1842 | running_loss 0.2364\n",
      "[GPT][E3] step 1320/1842 | running_loss 0.2432\n",
      "[GPT][E3] step 1360/1842 | running_loss 0.2380\n",
      "[GPT][E3] step 1400/1842 | running_loss 0.2460\n",
      "[GPT][E3] step 1440/1842 | running_loss 0.2408\n",
      "[GPT][E3] step 1480/1842 | running_loss 0.2361\n",
      "[GPT][E3] step 1520/1842 | running_loss 0.2406\n",
      "[GPT][E3] step 1560/1842 | running_loss 0.2409\n",
      "[GPT][E3] step 1600/1842 | running_loss 0.2435\n",
      "[GPT][E3] step 1640/1842 | running_loss 0.2444\n",
      "[GPT][E3] step 1680/1842 | running_loss 0.2492\n",
      "[GPT][E3] step 1720/1842 | running_loss 0.2443\n",
      "[GPT][E3] step 1760/1842 | running_loss 0.2472\n",
      "[GPT][E3] step 1800/1842 | running_loss 0.2390\n",
      "[GPT][E3] step 1840/1842 | running_loss 0.2446\n"
     ]
    }
   ],
   "source": [
    "EPOCHS             = 3\n",
    "LR                 = 5e-5\n",
    "WEIGHT_DECAY       = 0.01\n",
    "GRAD_ACCUM_STEPS   = 8\n",
    "MAX_GRAD_NORM      = 1.0\n",
    "WARMUP_RATIO       = 0.06\n",
    "\n",
    "# Optimizer with weight decay on non-bias/LayerNorm\n",
    "decay, nodecay = [], []\n",
    "for n,p in gpt.named_parameters():\n",
    "    (decay if not any(nd in n for nd in [\"bias\",\"LayerNorm.weight\"]) else nodecay).append(p)\n",
    "gpt_optim = AdamW([{\"params\": decay, \"weight_decay\": WEIGHT_DECAY},\n",
    "                   {\"params\": nodecay, \"weight_decay\": 0.0}], lr=LR, fused=True)\n",
    "\n",
    "updates_per_epoch = math.ceil(len(gpt_train_loader) / GRAD_ACCUM_STEPS)\n",
    "total_updates     = EPOCHS * updates_per_epoch\n",
    "warmup_steps      = max(1, int(WARMUP_RATIO * total_updates))\n",
    "\n",
    "gpt_sched = get_linear_schedule_with_warmup(gpt_optim, warmup_steps, total_updates)\n",
    "scaler    = torch.amp.GradScaler(enabled=(device.type==\"cuda\"))\n",
    "\n",
    "def train_gpt():\n",
    "    global_step = 0\n",
    "    gpt.train()\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        running = 0.0\n",
    "        gpt_optim.zero_grad(set_to_none=True)\n",
    "        for step, batch in enumerate(gpt_train_loader, 1):\n",
    "            batch = {k: v.to(device, non_blocking=True) for k, v in batch.items()}\n",
    "            # PyTorch 2.x autocast\n",
    "            with torch.amp.autocast(device_type=device.type, enabled=(device.type==\"cuda\")):\n",
    "                out  = gpt(**batch)                 # labels mask the prompt to -100\n",
    "                loss = out.loss / GRAD_ACCUM_STEPS\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            running += loss.item()\n",
    "\n",
    "            if step % GRAD_ACCUM_STEPS == 0:\n",
    "                scaler.unscale_(gpt_optim)\n",
    "                clip_grad_norm_(gpt.parameters(), MAX_GRAD_NORM)\n",
    "                scaler.step(gpt_optim); scaler.update()\n",
    "                gpt_optim.zero_grad(set_to_none=True)\n",
    "                gpt_sched.step()\n",
    "                global_step += 1\n",
    "\n",
    "            LOG_EVERY = max(10, GRAD_ACCUM_STEPS * 5)\n",
    "\n",
    "            if step % LOG_EVERY == 0:\n",
    "                print(f\"[GPT][E{epoch}] step {step}/{len(gpt_train_loader)} | running_loss {running/LOG_EVERY:.4f}\")\n",
    "                running = 0.0\n",
    "\n",
    "\n",
    "train_gpt()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed98645c",
   "metadata": {},
   "source": [
    "### GPT Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "27fcf96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVAL tokenizer (left padding)\n",
    "gpt_tok_eval = AutoTokenizer.from_pretrained(GPT_NAME, padding_side=\"left\")\n",
    "if gpt_tok_eval.pad_token is None:\n",
    "    gpt_tok_eval.pad_token = gpt_tok_eval.eos_token\n",
    "\n",
    "# Left-pad collator for decoder-only models\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List\n",
    "import torch\n",
    "\n",
    "@dataclass\n",
    "class GPTLeftPadCollator:\n",
    "    tokenizer: any\n",
    "    label_pad_id: int = -100\n",
    "    def __call__(self, features: List[Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:\n",
    "        pad_id = self.tokenizer.pad_token_id\n",
    "        bs = len(features)\n",
    "        max_len = max(f[\"input_ids\"].size(0) for f in features)\n",
    "\n",
    "        input_ids  = torch.full((bs, max_len), pad_id, dtype=torch.long)\n",
    "        attn_mask  = torch.zeros((bs, max_len), dtype=torch.long)\n",
    "        labels_out = torch.full((bs, max_len), self.label_pad_id, dtype=torch.long)\n",
    "\n",
    "        for i, f in enumerate(features):\n",
    "            L = f[\"input_ids\"].size(0)\n",
    "            input_ids[i,  -L:] = f[\"input_ids\"]\n",
    "            attn_mask[i,  -L:] = f[\"attention_mask\"]\n",
    "            labels_out[i,  -L:] = f[\"labels\"]\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attn_mask, \"labels\": labels_out}\n",
    "\n",
    "gpt_collator_eval = GPTLeftPadCollator(tokenizer=gpt_tok_eval)\n",
    "\n",
    "gpt_val_loader = DataLoader(\n",
    "    gpt_val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    collate_fn=gpt_collator_eval, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1676551a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_gpt(\n",
    "    model,\n",
    "    loader,\n",
    "    tokenizer,\n",
    "    max_new_tokens=96,\n",
    "    num_beams=1,\n",
    "    limit=None,\n",
    "    pad_to_multiple_of=None,     # None to disable; 8 helps tensor cores\n",
    "    min_new_tokens=8,\n",
    "    no_repeat_ngram_size=3,   # small repetition guard; set 0/None to disable\n",
    "    do_bertscore=False,        # turn off if you want max speed\n",
    "):\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    # Speed up AR decoding\n",
    "    was_ckpt = getattr(model, \"is_gradient_checkpointing\", False)\n",
    "    if was_ckpt:\n",
    "        model.gradient_checkpointing_disable()\n",
    "    if hasattr(model.config, \"use_cache\"):\n",
    "        model.config.use_cache = True\n",
    "\n",
    "    model.eval()\n",
    "    all_preds, all_refs = [], []\n",
    "    seen = 0\n",
    "\n",
    "    with torch.inference_mode(), torch.amp.autocast(device_type=device.type, enabled=(device.type==\"cuda\")):\n",
    "        for batch in loader:\n",
    "            if limit is not None and seen >= limit:\n",
    "                break\n",
    "\n",
    "            # ---- keep labels on CPU (we only need them to decode refs)\n",
    "            labs_cpu = batch[\"labels\"]  # (dont move to GPU)\n",
    "\n",
    "            # Move inputs to GPU once\n",
    "            ids  = batch[\"input_ids\"].to(device, non_blocking=True).long()\n",
    "            attn = batch[\"attention_mask\"].to(device, non_blocking=True).long()\n",
    "\n",
    "            # Sort by length to reduce padding/FLOPs (done on GPU)\n",
    "            lengths = attn.sum(dim=1)\n",
    "            order   = torch.argsort(lengths)          # GPU\n",
    "            inv     = torch.empty_like(order); inv[order] = torch.arange(order.numel(), device=device)\n",
    "\n",
    "            ids  = ids[order]\n",
    "            attn = attn[order]\n",
    "            lens_sorted = lengths[order]\n",
    "\n",
    "            # Optional: LEFT-pad to multiple of 8 (do it on GPU)\n",
    "            if pad_to_multiple_of:\n",
    "                pad_to = ((ids.size(1) + pad_to_multiple_of - 1) // pad_to_multiple_of) * pad_to_multiple_of\n",
    "                if pad_to > ids.size(1):\n",
    "                    left_pad = pad_to - ids.size(1)\n",
    "                    ids  = F.pad(ids,  (left_pad, 0), value=tokenizer.pad_token_id)\n",
    "                    attn = F.pad(attn, (left_pad, 0), value=0)\n",
    "\n",
    "            # Compute safe new-token budget for the whole sub-batch\n",
    "            max_ctx = getattr(tokenizer, \"model_max_length\", 1024) or 1024\n",
    "            longest_prompt = int(attn.sum(dim=1).max().item())\n",
    "            max_new = max(1, min(max_new_tokens, max_ctx - longest_prompt))\n",
    "\n",
    "            # ---- One batched generate() on GPU\n",
    "            gen = model.generate(\n",
    "                input_ids=ids,\n",
    "                attention_mask=attn,\n",
    "                max_new_tokens=max_new,\n",
    "                min_new_tokens=min_new_tokens,\n",
    "                num_beams=num_beams,          # 1 = greedy\n",
    "                do_sample=False,\n",
    "                no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "            )  # [B, prompt+gen] on GPU\n",
    "\n",
    "            # Slice completions by *padded input width* (padding-agnostic)\n",
    "            start = ids.size(1)                  # same for the whole sorted sub-batch\n",
    "            comps = gen[:, start:]               # still on GPU\n",
    "\n",
    "            # Decode preds (minimal CPU touch)\n",
    "            # Move only the completions we need to CPU for decoding\n",
    "            pred_texts_sorted = tokenizer.batch_decode(comps.cpu(), skip_special_tokens=True)\n",
    "\n",
    "            # Decode refs on CPU (labels never moved to GPU)\n",
    "            ref_texts_sorted = [\n",
    "                tokenizer.decode(l[l != -100], skip_special_tokens=True).strip()\n",
    "                for l in labs_cpu[order.cpu()]\n",
    "            ]\n",
    "\n",
    "            # Restore original order\n",
    "            inv_cpu = inv.cpu().tolist()\n",
    "            pred_texts = [pred_texts_sorted[k] for k in inv_cpu]\n",
    "            ref_texts  = [ref_texts_sorted[k]  for k in inv_cpu]\n",
    "\n",
    "            all_preds.extend(pred_texts)\n",
    "            all_refs.extend(ref_texts)\n",
    "            seen += len(pred_texts)\n",
    "\n",
    "            if limit is not None and seen >= limit:\n",
    "                break\n",
    "\n",
    "    # Restore training-time flags\n",
    "    if hasattr(model.config, \"use_cache\"):\n",
    "        model.config.use_cache = False\n",
    "    if was_ckpt:\n",
    "        model.gradient_checkpointing_enable()\n",
    "\n",
    "    # Filter empty pairs for metrics stability\n",
    "    pairs = [(p, r) for p, r in zip(all_preds, all_refs) if p and r]\n",
    "    if not pairs:\n",
    "        return all_preds, all_refs, {\"rouge1\":0, \"rouge2\":0, \"rougeL\":0}, 0.0\n",
    "    preds_f, refs_f = map(list, zip(*pairs))\n",
    "\n",
    "    # ROUGE (lightweight)  requires strings (CPU)\n",
    "    rouge = compute_rouge_light(preds_f, refs_f)\n",
    "\n",
    "    # (Optional) BERTScore  can be slow; toggle with do_bertscore\n",
    "    bert_f1 = 0.0\n",
    "    if do_bertscore:\n",
    "        try:\n",
    "            import evaluate\n",
    "            bertscore = evaluate.load(\"bertscore\")\n",
    "            bs = bertscore.compute(predictions=preds_f, references=refs_f, lang=\"en\")\n",
    "            bert_f1 = float(sum(bs[\"f1\"]) / len(bs[\"f1\"]))\n",
    "        except Exception:\n",
    "            bert_f1 = 0.0\n",
    "\n",
    "    return all_preds, all_refs, rouge, bert_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e517318e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.4268, 'rouge2': 0.4054, 'rougeL': 0.4255} | BERTScore F1: 0.0\n",
      "Saved: gpt_val_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# Run full eval (set limit to e.g. 200 to sample quickly)\n",
    "preds, refs, rouge_metrics, bert_f1 = evaluate_gpt(\n",
    "    gpt, gpt_val_loader, gpt_tok_eval, max_new_tokens=96, num_beams=1, limit=None\n",
    ")\n",
    "\n",
    "print({k: round(v, 4) for k, v in rouge_metrics.items()}, \"| BERTScore F1:\", round(bert_f1, 4))\n",
    "\n",
    "pd.DataFrame({\"pred\": preds, \"ref\": refs}).to_csv(\"gpt_val_predictions.csv\", index=False)\n",
    "print(\"Saved: gpt_val_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b5b5d869",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./checkpoints/distilgpt2-samsum\\\\tokenizer_config.json',\n",
       " './checkpoints/distilgpt2-samsum\\\\special_tokens_map.json',\n",
       " './checkpoints/distilgpt2-samsum\\\\vocab.json',\n",
       " './checkpoints/distilgpt2-samsum\\\\merges.txt',\n",
       " './checkpoints/distilgpt2-samsum\\\\added_tokens.json',\n",
       " './checkpoints/distilgpt2-samsum\\\\tokenizer.json')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt.save_pretrained(\"./checkpoints/distilgpt2-samsum\")\n",
    "gpt_tok.save_pretrained(\"./checkpoints/distilgpt2-samsum\")\n",
    "\n",
    "# Later:\n",
    "# gpt_tok = AutoTokenizer.from_pretrained(\"./checkpoints/distilgpt2-samsum\")\n",
    "# gpt     = AutoModelForCausalLM.from_pretrained(\"./checkpoints/distilgpt2-samsum\").to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f3c88c",
   "metadata": {},
   "source": [
    "### Samples from the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba5e784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- GPT samples ---\n",
      "\n",
      "[GPT] Example 1\n",
      "PRED:  Carol will book them.    Charles will pay a little more for the better seats.  Carol will let Carol know.  Charles is going to the Opera next month and will book the tickets.  He will book Carmen for the opera.  She will book her tickets. Carol is going on a trip to the theatre next month, so she will book a few more seats. Carol has to book the seats. She will be going to a theatre next year. \n",
      "REF : will book tickets for Carmen for himself and Carol.\n",
      "\n",
      "[GPT] Example 2\n",
      "PRED:  Jenny will let her in.    Jenny will get her flu.  Jenny has a key.  She will let herself in so she will take her key out.  Joins her to the shop in the morning.  The key is in the door.  It will be taken by Jenny.  He will let Jenny in. Joins Jenny in the afternoon.  They will have a cold and they will have flu. Join's key is taken by Jo\n",
      "REF : is coming down with a cold. Sue is doing grocery shopping for Jenny.\n",
      "\n",
      "[GPT] Example 3\n",
      "PRED:  Oliver is fed up.    Oliver is going to finish his paper.  Oliver finishes his paper in 10 seconds.  He finishes his papers in 10.  Liam is fed-up.  Oliver finishes his work in 10 sec.     \n",
      "Lionel: Liam is hungry. Oliver finishes the paper in ten seconds. Oliver finished his paper and is hungry, so he finishes his writing. \n",
      "Nelon:\n",
      "REF : and Oliver will meet in the hall in 10 minutes to have a break.\n",
      "\n",
      "[GPT] Example 4\n",
      "PRED:     Emma will plan another time.  Emma is planning another time, but will not go.  George will not be able to go again.  She will not have any more fun. Emma will not want to go back.  Emma will not come back. Emma is going back. She will be back.   Emmma will be home.  Emmia will be away. Emmia is going home. Em\n",
      "REF : was late and missed Andy's song, but she still had fun.\n",
      "\n",
      "[GPT] Example 5\n",
      "PRED:     Marc will let Rafael know about the gym on Monday.  Marc is going to let Rafael decide on Monday afternoon.  Rafael will let him know about his training on Monday morning.  Rafael will let Marc know about training on Tuesday afternoon. Marc will tell Rafael about training.  He will tell Rafael about training and Rafael's training. Marc is not going to go there.     Marc will tell Marc about training\n",
      "REF : overdid his training after a break yesterday and is too sore to go to the gym today. If fine by Tuesday, he will let Rafael know about it on Monday evening.\n"
     ]
    }
   ],
   "source": [
    "def show_samples(df, k=5, seed=13):\n",
    "    import numpy as np\n",
    "    np.random.seed(seed)\n",
    "    idx = np.random.choice(len(df), size=min(k, len(df)), replace=False)\n",
    "    for i, j in enumerate(idx, 1):\n",
    "        row = df.iloc[j]\n",
    "        print(f\"\\n[{row['model']}] Example {i}\")\n",
    "        print(\"PRED:\", row[\"pred\"])\n",
    "        print(\"REF :\", row[\"ref\"])\n",
    "gpt_df = pd.DataFrame({\"pred\": preds, \"ref\": refs})\n",
    "gpt_df[\"model\"] = \"GPT\"\n",
    "\n",
    "print(\"\\n--- GPT samples ---\")\n",
    "show_samples(gpt_df,  k=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
