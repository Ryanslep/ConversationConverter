{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "451d46c6",
   "metadata": {},
   "source": [
    "# Acme Conversation Converter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9841206b",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "868cb2fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x196ff2d2cd0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Any, List\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BartForConditionalGeneration, get_linear_schedule_with_warmup\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4872320",
   "metadata": {},
   "source": [
    "### Load and Explore Samsum Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e32fb09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14731 818 819\n",
      "Train shape: (14731, 3) | Val shape: (818, 3)\n",
      "Sample rows:\n",
      "          id                                           dialogue  \\\n",
      "0  13818513  Amanda: I baked  cookies. Do you want some?\\nJ...   \n",
      "1  13728867  Olivia: Who are you voting for in this electio...   \n",
      "2  13681000  Tim: Hi, what's up?\\nKim: Bad mood tbh, I was ...   \n",
      "3  13730747  Edward: Rachel, I think I'm in ove with Bella....   \n",
      "4  13728094  Sam: hey  overheard rick say something\\nSam: i...   \n",
      "\n",
      "                                             summary  \n",
      "0  Amanda baked cookies and will bring Jerry some...  \n",
      "1  Olivia and Olivier are voting for liberals in ...  \n",
      "2  Kim may try the pomodoro technique recommended...  \n",
      "3  Edward thinks he is in love with Bella. Rachel...  \n",
      "4  Sam is confused, because he overheard Rick com...  \n"
     ]
    }
   ],
   "source": [
    "# Load Samsum dataset\n",
    "df = None\n",
    "for repo in [\"samsum\", \"knkarthick/samsum\"]:\n",
    "    try:\n",
    "        df = load_dataset(repo)\n",
    "        break\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "df_train = df[\"train\"].to_pandas()[[\"id\",\"dialogue\",\"summary\"]]\n",
    "df_val   = df[\"validation\"].to_pandas()[[\"id\",\"dialogue\",\"summary\"]]\n",
    "df_test  = df[\"test\"].to_pandas()[[\"id\",\"dialogue\",\"summary\"]] if \"test\" in df else None\n",
    "\n",
    "print(len(df_train), len(df_val), len(df_test) if df_test is not None else 0)\n",
    "\n",
    "print(\"Train shape:\", df_train.shape, \"| Val shape:\", df_val.shape)\n",
    "print(\"Sample rows:\\n\", df_train.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "22a6d588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Train word count summary ===\n",
      "       dialogue_word_count  summary_word_count\n",
      "count         14731.000000        14731.000000\n",
      "mean             93.792750           20.318444\n",
      "std              74.031937           11.153570\n",
      "min               7.000000            1.000000\n",
      "25%              39.000000           12.000000\n",
      "50%              73.000000           18.000000\n",
      "75%             128.000000           27.000000\n",
      "max             803.000000           64.000000\n",
      "\n",
      "=== Validation word count summary ===\n",
      "       dialogue_word_count  summary_word_count\n",
      "count           818.000000          818.000000\n",
      "mean             91.641809           20.283619\n",
      "std              74.479672           11.211454\n",
      "min              10.000000            3.000000\n",
      "25%              38.000000           12.000000\n",
      "50%              70.000000           18.000000\n",
      "75%             127.000000           26.000000\n",
      "max             540.000000           59.000000\n"
     ]
    }
   ],
   "source": [
    "def add_len_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[\"dialogue_word_count\"] = df[\"dialogue\"].str.split().apply(len)\n",
    "    df[\"summary_word_count\"]  = df[\"summary\"].str.split().apply(len)\n",
    "    return df\n",
    "\n",
    "eda_train = add_len_cols(df_train)\n",
    "eda_val   = add_len_cols(df_val)\n",
    "\n",
    "print(\"\\n=== Train word count summary ===\")\n",
    "print(eda_train[[\"dialogue_word_count\", \"summary_word_count\"]].describe())\n",
    "\n",
    "print(\"\\n=== Validation word count summary ===\")\n",
    "print(eda_val[[\"dialogue_word_count\", \"summary_word_count\"]].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4434951",
   "metadata": {},
   "source": [
    "### BART Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3dba3b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batches/epoch: 921 val: 52\n"
     ]
    }
   ],
   "source": [
    "# TOKENIZERS (BERT + GPT)\n",
    "# BART tokenizer & model\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "bart_name = \"sshleifer/distilbart-cnn-12-6\"\n",
    "bart_tok = AutoTokenizer.from_pretrained(bart_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(bart_name).to(device)\n",
    "\n",
    "model.gradient_checkpointing_disable()   # saves memory\n",
    "model.config.use_cache = False\n",
    "\n",
    "\n",
    "class BartSumDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_source_len=512, max_target_len=128):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tok = tokenizer\n",
    "        self.src_len = max_source_len\n",
    "        self.tgt_len = max_target_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        row = self.df.iloc[idx]\n",
    "        dialogue = str(row[\"dialogue\"])\n",
    "        summary  = str(row[\"summary\"])\n",
    "\n",
    "        # Encode source (dialogue)\n",
    "        enc = self.tok(\n",
    "            dialogue,\n",
    "            max_length=self.src_len,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",          # returns torch.Tensor\n",
    "        )\n",
    "\n",
    "        # Encode target (summary)\n",
    "        dec = self.tok(\n",
    "            text_target=summary,\n",
    "            max_length=self.tgt_len,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",          # returns torch.Tensor\n",
    "        )\n",
    "\n",
    "        labels = dec[\"input_ids\"].clone()\n",
    "        labels[labels == self.tok.pad_token_id] = -100\n",
    "\n",
    "        # Squeeze away the batch dim since return_tensors=\"pt\" adds it\n",
    "        return {\n",
    "            \"input_ids\": enc[\"input_ids\"].squeeze(0),         # torch.LongTensor\n",
    "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": labels.squeeze(0),\n",
    "        }\n",
    "\n",
    "@dataclass\n",
    "class BartDataCollator:\n",
    "    tokenizer: any\n",
    "    label_pad_token_id: int = -100\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:\n",
    "        # Convert any numpy arrays in labels to tensors safely\n",
    "        for f in features:\n",
    "            if isinstance(f[\"labels\"], np.ndarray):\n",
    "                f[\"labels\"] = torch.tensor(f[\"labels\"], dtype=torch.int64)\n",
    "            elif isinstance(f[\"labels\"], list) and isinstance(f[\"labels\"][0], np.ndarray):\n",
    "                f[\"labels\"] = torch.tensor(np.array(f[\"labels\"]), dtype=torch.int64)\n",
    "\n",
    "        # Use Hugging Face helper to pad properly\n",
    "        batch = self.tokenizer.pad(\n",
    "            features,\n",
    "            padding=True,\n",
    "            max_length=None,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        # Replace padding token IDs in labels with -100 so they are ignored in loss\n",
    "        if \"labels\" in batch:\n",
    "            batch[\"labels\"] = torch.where(\n",
    "                batch[\"labels\"] == self.tokenizer.pad_token_id,\n",
    "                torch.tensor(self.label_pad_token_id),\n",
    "                batch[\"labels\"],\n",
    "            )\n",
    "\n",
    "        return batch\n",
    "\n",
    "# lengths & batch (full training settings)\n",
    "MAX_SOURCE_LEN  = 384\n",
    "MAX_TARGET_LEN = 96\n",
    "BATCH_SIZE   = 16\n",
    "NUM_WORKERS  = 0\n",
    "PIN_MEMORY   = torch.cuda.is_available()\n",
    "PREFETCH     = None\n",
    "\n",
    "# For TRAINING speed:\n",
    "model.config.use_cache = False        # cache hurts training speed/mem\n",
    "model.gradient_checkpointing_disable()# disable for speed (enable only if you need memory)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "torch.backends.cuda.enable_flash_sdp(True)\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(True)\n",
    "torch.backends.cuda.enable_math_sdp(True)\n",
    "\n",
    "train_ds = BartSumDataset(df_train, bart_tok, MAX_SOURCE_LEN, MAX_TARGET_LEN)\n",
    "val_ds   = BartSumDataset(df_val,   bart_tok, MAX_SOURCE_LEN, MAX_TARGET_LEN)\n",
    "test_ds  = BartSumDataset(df_test,  bart_tok, MAX_SOURCE_LEN, MAX_TARGET_LEN) if df_test is not None else None\n",
    "\n",
    "pad_to_mult = 8 if torch.cuda.is_available() else None\n",
    "\n",
    "collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=bart_tok,\n",
    "    model=model,                      # your BART model\n",
    "    label_pad_token_id=-100,\n",
    "    pad_to_multiple_of=pad_to_mult\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          collate_fn=collator, num_workers=NUM_WORKERS,\n",
    "                          pin_memory=PIN_MEMORY, prefetch_factor=PREFETCH, persistent_workers=False)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          collate_fn=collator, num_workers=NUM_WORKERS,\n",
    "                          pin_memory=PIN_MEMORY, prefetch_factor=PREFETCH, persistent_workers=False)\n",
    "test_loader  = (DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                           collate_fn=collator, num_workers=NUM_WORKERS, \n",
    "                           pin_memory=PIN_MEMORY, prefetch_factor=PREFETCH, persistent_workers=False)\n",
    "                if test_ds is not None else None)\n",
    "print(\"Batches/epoch:\", len(train_loader), \"val:\", len(val_loader))\n",
    "\n",
    "# Prompt template for causal LM\n",
    "PROMPT_PREFIX = \"Summarize the following dialogue:\\n\"\n",
    "PROMPT_SUFFIX = \"\\nSummary:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "68c7ebbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = BartForConditionalGeneration.from_pretrained(bart_name).to(device)\n",
    "model.config.pad_token_id = bart_tok.pad_token_id\n",
    "model.config.bos_token_id = bart_tok.bos_token_id\n",
    "model.config.eos_token_id = bart_tok.eos_token_id\n",
    "\n",
    "# Epochs / lr / regularization\n",
    "EPOCHS            = 3\n",
    "LR                = 3e-5\n",
    "WEIGHT_DECAY      = 0.01\n",
    "GRAD_ACCUM_STEPS  = 8      # effective batch ~= BATCH_SIZE * GRAD_ACCUM_STEPS\n",
    "MAX_GRAD_NORM     = 1.0\n",
    "WARMUP_RATIO      = 0.06\n",
    "\n",
    "# Optimizer (AdamW) with weight decay on non-bias/LayerNorm\n",
    "decay, no_decay = [], []\n",
    "for n,p in model.named_parameters():\n",
    "    (decay if not any(nd in n for nd in [\"bias\",\"LayerNorm.weight\"]) else no_decay).append(p)\n",
    "optimizer = AdamW(\n",
    "    [{\"params\": decay, \"weight_decay\": WEIGHT_DECAY},\n",
    "     {\"params\": no_decay, \"weight_decay\": 0.0}],\n",
    "    lr=LR,\n",
    "    fused=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "num_update_steps_per_epoch = math.ceil(len(train_loader) / GRAD_ACCUM_STEPS)\n",
    "t_total = EPOCHS * num_update_steps_per_epoch\n",
    "num_warmup = max(1, int(WARMUP_RATIO * t_total))\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup, t_total)\n",
    "\n",
    "scaler = torch.amp.GradScaler(enabled=(device.type==\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6246ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6f355495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Model device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Model device:\", next(model.parameters()).device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "223222b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[probe] steps=2, sec=32.7, tokens/sec=349\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def measure_steps(model, loader, steps=10):\n",
    "    model.train()\n",
    "    it = iter(loader)\n",
    "    t0 = time.time(); toks = 0\n",
    "    for i in range(steps):\n",
    "        batch = next(it)\n",
    "        batch = {k: v.to(model.device, non_blocking=True) for k,v in batch.items()}\n",
    "        with torch.amp.autocast(device_type=model.device.type, enabled=(model.device.type==\"cuda\")):\n",
    "            out = model(**batch)\n",
    "            loss = out.loss\n",
    "        loss.backward()\n",
    "        toks += batch[\"input_ids\"].numel()\n",
    "    dt = time.time()-t0\n",
    "    print(f\"[probe] steps={steps}, sec={dt:.1f}, tokens/sec={toks/dt:.0f}\")\n",
    "\n",
    "measure_steps(model, train_loader, steps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "706bb4cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[108], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m                 \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[E\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] step \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrunning\u001b[38;5;241m/\u001b[39m(GRAD_ACCUM_STEPS\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     27\u001b[0m                 running \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m---> 29\u001b[0m train()\n",
      "Cell \u001b[1;32mIn[108], line 14\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m     out \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[0;32m     12\u001b[0m     loss \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;241m/\u001b[39m GRAD_ACCUM_STEPS\n\u001b[1;32m---> 14\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     15\u001b[0m running \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m GRAD_ACCUM_STEPS \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\ryans\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    628\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ryans\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[0;32m    348\u001b[0m     tensors,\n\u001b[0;32m    349\u001b[0m     grad_tensors_,\n\u001b[0;32m    350\u001b[0m     retain_graph,\n\u001b[0;32m    351\u001b[0m     create_graph,\n\u001b[0;32m    352\u001b[0m     inputs,\n\u001b[0;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    355\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ryans\\anaconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    824\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    825\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    global_step = 0\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        model.train()\n",
    "        running = 0.0\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        for step, batch in enumerate(train_loader, 1):\n",
    "            batch = {k: v.to(device, non_blocking=True) for k, v in batch.items()}\n",
    "            with torch.amp.autocast(device_type=device.type, enabled=(device.type==\"cuda\")):\n",
    "                out = model(**batch)\n",
    "                loss = out.loss / GRAD_ACCUM_STEPS\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            running += loss.item()\n",
    "\n",
    "            if step % GRAD_ACCUM_STEPS == 0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
    "                scaler.step(optimizer); scaler.update()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                scheduler.step()\n",
    "                global_step += 1\n",
    "\n",
    "            if step % (GRAD_ACCUM_STEPS*2) == 0:\n",
    "                print(f\"[E{epoch}] step {step}/{len(train_loader)} | loss {running/(GRAD_ACCUM_STEPS*2):.4f}\")\n",
    "                running = 0.0\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5fd10e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "_word_re = re.compile(r\"\\w+('\\w+)?\", re.UNICODE)\n",
    "\n",
    "def _tokenize(s: str):\n",
    "    return _word_re.findall(s.lower())\n",
    "\n",
    "def _f1(overlap, ref_count, pred_count):\n",
    "    if overlap == 0: return 0.0\n",
    "    precision = overlap / pred_count if pred_count else 0.0\n",
    "    recall    = overlap / ref_count  if ref_count  else 0.0\n",
    "    if precision + recall == 0: return 0.0\n",
    "    return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "def _ngram_counts(tokens, n):\n",
    "    return Counter(tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1))\n",
    "\n",
    "def _rouge_n(ref_tokens, pred_tokens, n):\n",
    "    ref = _ngram_counts(ref_tokens, n)\n",
    "    pred = _ngram_counts(pred_tokens, n)\n",
    "    overlap = sum((ref & pred).values())\n",
    "    return _f1(overlap, sum(ref.values()), sum(pred.values()))\n",
    "\n",
    "def _lcs_len(a, b):\n",
    "    # classic DP LCS length\n",
    "    m, n = len(a), len(b)\n",
    "    dp = [0]*(n+1)\n",
    "    for i in range(1, m+1):\n",
    "        prev = 0\n",
    "        for j in range(1, n+1):\n",
    "            tmp = dp[j]\n",
    "            if a[i-1] == b[j-1]:\n",
    "                dp[j] = prev + 1\n",
    "            else:\n",
    "                dp[j] = max(dp[j], dp[j-1])\n",
    "            prev = tmp\n",
    "    return dp[-1]\n",
    "\n",
    "def _rouge_l(ref_tokens, pred_tokens):\n",
    "    lcs = _lcs_len(ref_tokens, pred_tokens)\n",
    "    return _f1(lcs, len(ref_tokens), len(pred_tokens))\n",
    "\n",
    "def compute_rouge_light(preds, refs):\n",
    "    \"\"\"NLTK-free ROUGE: returns {'rouge1','rouge2','rougeL'} (F1).\"\"\"\n",
    "    assert len(preds) == len(refs)\n",
    "    r1=r2=rl=0.0\n",
    "    for p, r in zip(preds, refs):\n",
    "        pt = _tokenize(p); rt = _tokenize(r)\n",
    "        r1 += _rouge_n(rt, pt, 1)\n",
    "        r2 += _rouge_n(rt, pt, 2)\n",
    "        rl += _rouge_l(rt, pt)\n",
    "    n = max(1, len(preds))\n",
    "    return {\"rouge1\": r1/n, \"rouge2\": r2/n, \"rougeL\": rl/n}\n",
    "\n",
    "bertscore_metric = evaluate.load(\"bertscore\")\n",
    "\n",
    "def get_bertscore(preds, refs, lang=\"en\"):\n",
    "    \"\"\"\n",
    "    Compute BERTScore F1 between predictions and references.\n",
    "    Args:\n",
    "        preds (list[str]): Model-generated summaries\n",
    "        refs (list[str]): Reference summaries\n",
    "        lang (str): Language code (default: English)\n",
    "    Returns:\n",
    "        float: Average F1 score\n",
    "    \"\"\"\n",
    "    results = bertscore_metric.compute(predictions=preds, references=refs, lang=lang)\n",
    "    return float(sum(results[\"f1\"]) / len(results[\"f1\"]))\n",
    "    \n",
    "def basic_diag(preds, refs):\n",
    "    import numpy as np, re\n",
    "    def wc(s): return len(re.findall(r\"\\w+\", s))\n",
    "    len_pred = np.array([wc(p) for p in preds]); len_ref = np.array([wc(r) for r in refs])\n",
    "    ratios   = len_pred / np.maximum(1, len_ref)\n",
    "    return {\n",
    "        \"avg_len_pred\": float(len_pred.mean()),\n",
    "        \"avg_len_ref\":  float(len_ref.mean()),\n",
    "        \"len_ratio_mean\": float(ratios.mean()),\n",
    "        \"len_ratio_median\": float(np.median(ratios)),\n",
    "    }\n",
    "\n",
    "def decode_refs_from_labels(tokenizer, label_tensor_batch):\n",
    "    refs = []\n",
    "    for t in label_tensor_batch:\n",
    "        t = t[t != -100]\n",
    "        refs.append(tokenizer.decode(t, skip_special_tokens=True).strip())\n",
    "    return refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "90b561ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import torch\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_bart(model, loader, tokenizer, max_len=128, num_beams=4, limit=None):\n",
    "    model.eval()\n",
    "    preds, refs = [], []\n",
    "    for i, batch in enumerate(loader):\n",
    "        if limit is not None and i >= limit: break\n",
    "        inp = batch[\"input_ids\"].to(model.device)\n",
    "        att = batch[\"attention_mask\"].to(model.device)\n",
    "        gen = model.generate(\n",
    "            input_ids=inp, attention_mask=att,\n",
    "            max_length=max_len, num_beams=num_beams, early_stopping=True,\n",
    "            pad_token_id=tokenizer.pad_token_id, eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "        preds.extend(tokenizer.batch_decode(gen, skip_special_tokens=True))\n",
    "        refs.extend(decode_refs_from_labels(tokenizer, batch[\"labels\"]))\n",
    "    r = compute_rouge_light(preds, refs)\n",
    "    bs = get_bertscore(preds, refs) \n",
    "    diag = basic_diag(preds, refs)\n",
    "    df = pd.DataFrame({\"model\":\"BART\", \"pred\": preds, \"ref\": refs})\n",
    "    return r, bs, diag, df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc7738e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.5949, 'rouge2': 0.5717, 'rougeL': 0.5917}\n",
      "Saved: bart_val_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# Use existing loaders;\n",
    "MAX_TARGET_LEN = 96  # keep consistent with training\n",
    "\n",
    "preds, refs, scores = evaluate_bart(\n",
    "    model, val_loader, bart_tok,\n",
    "    max_len=MAX_TARGET_LEN,\n",
    "    num_beams=4,        # try 2 for speed, or 1 for greedy\n",
    "    limit= None          # set to e.g. 100 to do a quick subset\n",
    ")\n",
    "\n",
    "print({k: round(v, 4) for k, v in scores.items()})\n",
    "\n",
    "# Save side-by-side for inspection\n",
    "import pandas as pd\n",
    "pd.DataFrame({\"pred\": preds, \"ref\": refs}).to_csv(\"bart_val_predictions.csv\", index=False)\n",
    "print(\"Saved: bart_val_predictions.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6183daad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e23abc28ce144e7824e41d97df6302e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c52806338eef4cdb82dba215e3853571",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba1afdfd169b40dfa6e3ac8954e28482",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d793a942336c498e9a89461fde45b9c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3812da3687041cca03d31029d7d3e16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac6f73abb6024db1a3bbbd35efa8d210",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f51e314f6f5d40ad9721396fcf08c9c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore F1: 0.9003064607641225\n"
     ]
    }
   ],
   "source": [
    "bertscore = evaluate.load(\"bertscore\")\n",
    "scores = bertscore.compute(predictions=preds, references=refs, lang=\"en\")\n",
    "print(\"BERTScore F1:\", sum(scores[\"f1\"])/len(scores[\"f1\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35358c5a",
   "metadata": {},
   "source": [
    "#### GPT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "389f9fa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPT_NAME = \"distilgpt2\"\n",
    "\n",
    "gpt_tok = AutoTokenizer.from_pretrained(GPT_NAME)\n",
    "if gpt_tok.pad_token is None:  # ensure proper padding & clean decoding\n",
    "    gpt_tok.pad_token = gpt_tok.eos_token\n",
    "\n",
    "gpt = AutoModelForCausalLM.from_pretrained(GPT_NAME)\n",
    "gpt.resize_token_embeddings(len(gpt_tok))  # if pad token was added\n",
    "gpt.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2f9f6145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building GPT Dataset\n",
    "class GPTDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for auto-regressive LM summarization.\n",
    "    Builds: prompt = PREFIX + dialogue + SUFFIX + \" \" + summary\n",
    "    Masks prompt tokens with -100 so only summary contributes to loss.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, tokenizer, max_length=1024,\n",
    "                 prompt_prefix=\"Summarize the following dialogue:\\n\",\n",
    "                 prompt_suffix=\"\\nSummary:\"):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tok = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.prefix = prompt_prefix\n",
    "        self.suffix = prompt_suffix\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        dialogue = str(row[\"dialogue\"])\n",
    "        summary  = str(row[\"summary\"])\n",
    "\n",
    "        # prompt = instruction + dialogue + suffix\n",
    "        prompt = f\"{self.prefix}{dialogue}{self.suffix} \"\n",
    "        tok_prompt = self.tok(prompt, add_special_tokens=False,\n",
    "                              truncation=True, max_length=self.max_length)\n",
    "        tok_full   = self.tok(prompt + summary, add_special_tokens=False,\n",
    "                              truncation=True, max_length=self.max_length)\n",
    "\n",
    "        ids = tok_full[\"input_ids\"][: self.max_length]\n",
    "        attn = [1] * len(ids)\n",
    "\n",
    "        # mask out prompt portion\n",
    "        prompt_len = min(len(tok_prompt[\"input_ids\"]), len(ids))\n",
    "        labels = [-100] * prompt_len + ids[prompt_len:]\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(ids, dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(attn, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "21aecba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT Collator\n",
    "@dataclass\n",
    "class GPTCollator:\n",
    "    tokenizer: any\n",
    "    label_pad_id: int = -100\n",
    "\n",
    "    def __call__(self, features):\n",
    "        input_ids = [f[\"input_ids\"] for f in features]\n",
    "        attn      = [f[\"attention_mask\"] for f in features]\n",
    "        labels    = [f[\"labels\"] for f in features]\n",
    "\n",
    "        input_ids_padded = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        attn_padded = torch.nn.utils.rnn.pad_sequence(\n",
    "            attn, batch_first=True, padding_value=0\n",
    "        )\n",
    "        labels_padded = torch.nn.utils.rnn.pad_sequence(\n",
    "            labels, batch_first=True, padding_value=self.label_pad_id\n",
    "        )\n",
    "\n",
    "        return {\"input_ids\": input_ids_padded,\n",
    "                \"attention_mask\": attn_padded,\n",
    "                \"labels\": labels_padded}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d973c2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT loaders ready: 1842 52\n"
     ]
    }
   ],
   "source": [
    "MAX_GPT_LEN  = 1024\n",
    "BATCH_SIZE   = 8\n",
    "NUM_WORKERS  = 0\n",
    "PIN_MEMORY   = torch.cuda.is_available()\n",
    "PREFETCH     = None\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")  # TF32 fast matmuls on Ampere+\n",
    "torch.backends.cuda.enable_flash_sdp(True)\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(True)\n",
    "torch.backends.cuda.enable_math_sdp(True)\n",
    "\n",
    "gpt.gradient_checkpointing_disable()\n",
    "gpt.config.use_cache = False\n",
    "\n",
    "gpt_train_ds = GPTDataset(df_train, gpt_tok, max_length=MAX_GPT_LEN,\n",
    "                                prompt_prefix=PROMPT_PREFIX, prompt_suffix=PROMPT_SUFFIX)\n",
    "gpt_val_ds   = GPTDataset(df_val,   gpt_tok, max_length=MAX_GPT_LEN,\n",
    "                                prompt_prefix=PROMPT_PREFIX, prompt_suffix=PROMPT_SUFFIX)\n",
    "\n",
    "gpt_collator = GPTCollator(tokenizer=gpt_tok)\n",
    "\n",
    "gpt_train_loader = DataLoader(\n",
    "    gpt_train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    collate_fn=gpt_collator, num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY, prefetch_factor=PREFETCH, persistent_workers=False\n",
    ")\n",
    "gpt_val_loader = DataLoader(\n",
    "    gpt_val_ds, batch_size=16, shuffle=False,\n",
    "    collate_fn=gpt_collator, num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY, prefetch_factor=PREFETCH, persistent_workers=False\n",
    ")\n",
    "print(\"GPT loaders ready:\", len(gpt_train_loader), len(gpt_val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "29d3d09d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[89], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m     dt \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;241m-\u001b[39mt0\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[probe] steps=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msteps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, sec=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdt\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, tokens/sec=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtoks\u001b[38;5;241m/\u001b[39mdt\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.0f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m measure_steps(gpt, gpt_train_loader, steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "Cell \u001b[1;32mIn[89], line 9\u001b[0m, in \u001b[0;36mmeasure_steps\u001b[1;34m(model, loader, steps)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(steps):\n\u001b[0;32m      8\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(it)\n\u001b[1;32m----> 9\u001b[0m     batch \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(device_type\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype, enabled\u001b[38;5;241m=\u001b[39m(model\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[0;32m     11\u001b[0m         out \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbatch)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def measure_steps(model, loader, steps=10):\n",
    "    model.train()\n",
    "    it = iter(loader)\n",
    "    t0 = time.time(); toks = 0\n",
    "    for i in range(steps):\n",
    "        batch = next(it)\n",
    "        batch = {k: v.to(model.device, non_blocking=True) for k,v in batch.items()}\n",
    "        with torch.amp.autocast(device_type=model.device.type, enabled=(model.device.type==\"cuda\")):\n",
    "            out = model(**batch)\n",
    "            loss = out.loss\n",
    "        loss.backward()\n",
    "        toks += batch[\"input_ids\"].numel()\n",
    "    dt = time.time()-t0\n",
    "    print(f\"[probe] steps={steps}, sec={dt:.1f}, tokens/sec={toks/dt:.0f}\")\n",
    "\n",
    "measure_steps(gpt, gpt_train_loader, steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e472d445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GPT][E1] step 40/1842 | running_loss 0.2598\n",
      "[GPT][E1] step 80/1842 | running_loss 0.2719\n",
      "[GPT][E1] step 120/1842 | running_loss 0.2621\n",
      "[GPT][E1] step 160/1842 | running_loss 0.2669\n",
      "[GPT][E1] step 200/1842 | running_loss 0.2719\n",
      "[GPT][E1] step 240/1842 | running_loss 0.2554\n",
      "[GPT][E1] step 280/1842 | running_loss 0.2733\n",
      "[GPT][E1] step 320/1842 | running_loss 0.2604\n",
      "[GPT][E1] step 360/1842 | running_loss 0.2614\n",
      "[GPT][E1] step 400/1842 | running_loss 0.2654\n",
      "[GPT][E1] step 440/1842 | running_loss 0.2721\n",
      "[GPT][E1] step 480/1842 | running_loss 0.2661\n",
      "[GPT][E1] step 520/1842 | running_loss 0.2672\n",
      "[GPT][E1] step 560/1842 | running_loss 0.2688\n",
      "[GPT][E1] step 600/1842 | running_loss 0.2658\n",
      "[GPT][E1] step 640/1842 | running_loss 0.2600\n",
      "[GPT][E1] step 680/1842 | running_loss 0.2654\n",
      "[GPT][E1] step 720/1842 | running_loss 0.2619\n",
      "[GPT][E1] step 760/1842 | running_loss 0.2615\n",
      "[GPT][E1] step 800/1842 | running_loss 0.2616\n",
      "[GPT][E1] step 840/1842 | running_loss 0.2708\n",
      "[GPT][E1] step 880/1842 | running_loss 0.2591\n",
      "[GPT][E1] step 920/1842 | running_loss 0.2636\n",
      "[GPT][E1] step 960/1842 | running_loss 0.2631\n",
      "[GPT][E1] step 1000/1842 | running_loss 0.2662\n",
      "[GPT][E1] step 1040/1842 | running_loss 0.2636\n",
      "[GPT][E1] step 1080/1842 | running_loss 0.2568\n",
      "[GPT][E1] step 1120/1842 | running_loss 0.2630\n",
      "[GPT][E1] step 1160/1842 | running_loss 0.2683\n",
      "[GPT][E1] step 1200/1842 | running_loss 0.2708\n",
      "[GPT][E1] step 1240/1842 | running_loss 0.2552\n",
      "[GPT][E1] step 1280/1842 | running_loss 0.2662\n",
      "[GPT][E1] step 1320/1842 | running_loss 0.2654\n",
      "[GPT][E1] step 1360/1842 | running_loss 0.2627\n",
      "[GPT][E1] step 1400/1842 | running_loss 0.2662\n",
      "[GPT][E1] step 1440/1842 | running_loss 0.2587\n",
      "[GPT][E1] step 1480/1842 | running_loss 0.2605\n",
      "[GPT][E1] step 1520/1842 | running_loss 0.2678\n",
      "[GPT][E1] step 1560/1842 | running_loss 0.2669\n",
      "[GPT][E1] step 1600/1842 | running_loss 0.2689\n",
      "[GPT][E1] step 1640/1842 | running_loss 0.2598\n",
      "[GPT][E1] step 1680/1842 | running_loss 0.2630\n",
      "[GPT][E1] step 1720/1842 | running_loss 0.2665\n",
      "[GPT][E1] step 1760/1842 | running_loss 0.2605\n",
      "[GPT][E1] step 1800/1842 | running_loss 0.2523\n",
      "[GPT][E1] step 1840/1842 | running_loss 0.2612\n",
      "[GPT][E2] step 40/1842 | running_loss 0.2581\n",
      "[GPT][E2] step 80/1842 | running_loss 0.2597\n",
      "[GPT][E2] step 120/1842 | running_loss 0.2480\n",
      "[GPT][E2] step 160/1842 | running_loss 0.2569\n",
      "[GPT][E2] step 200/1842 | running_loss 0.2445\n",
      "[GPT][E2] step 240/1842 | running_loss 0.2493\n",
      "[GPT][E2] step 280/1842 | running_loss 0.2519\n",
      "[GPT][E2] step 320/1842 | running_loss 0.2474\n",
      "[GPT][E2] step 360/1842 | running_loss 0.2511\n",
      "[GPT][E2] step 400/1842 | running_loss 0.2537\n",
      "[GPT][E2] step 440/1842 | running_loss 0.2541\n",
      "[GPT][E2] step 480/1842 | running_loss 0.2541\n",
      "[GPT][E2] step 520/1842 | running_loss 0.2467\n",
      "[GPT][E2] step 560/1842 | running_loss 0.2588\n",
      "[GPT][E2] step 600/1842 | running_loss 0.2532\n",
      "[GPT][E2] step 640/1842 | running_loss 0.2492\n",
      "[GPT][E2] step 680/1842 | running_loss 0.2536\n",
      "[GPT][E2] step 720/1842 | running_loss 0.2437\n",
      "[GPT][E2] step 760/1842 | running_loss 0.2479\n",
      "[GPT][E2] step 800/1842 | running_loss 0.2461\n",
      "[GPT][E2] step 840/1842 | running_loss 0.2518\n",
      "[GPT][E2] step 880/1842 | running_loss 0.2512\n",
      "[GPT][E2] step 920/1842 | running_loss 0.2432\n",
      "[GPT][E2] step 960/1842 | running_loss 0.2534\n",
      "[GPT][E2] step 1000/1842 | running_loss 0.2598\n",
      "[GPT][E2] step 1040/1842 | running_loss 0.2539\n",
      "[GPT][E2] step 1080/1842 | running_loss 0.2404\n",
      "[GPT][E2] step 1120/1842 | running_loss 0.2492\n",
      "[GPT][E2] step 1160/1842 | running_loss 0.2503\n",
      "[GPT][E2] step 1200/1842 | running_loss 0.2469\n",
      "[GPT][E2] step 1240/1842 | running_loss 0.2487\n",
      "[GPT][E2] step 1280/1842 | running_loss 0.2507\n",
      "[GPT][E2] step 1320/1842 | running_loss 0.2487\n",
      "[GPT][E2] step 1360/1842 | running_loss 0.2506\n",
      "[GPT][E2] step 1400/1842 | running_loss 0.2419\n",
      "[GPT][E2] step 1440/1842 | running_loss 0.2501\n",
      "[GPT][E2] step 1480/1842 | running_loss 0.2509\n",
      "[GPT][E2] step 1520/1842 | running_loss 0.2456\n",
      "[GPT][E2] step 1560/1842 | running_loss 0.2509\n",
      "[GPT][E2] step 1600/1842 | running_loss 0.2412\n",
      "[GPT][E2] step 1640/1842 | running_loss 0.2456\n",
      "[GPT][E2] step 1680/1842 | running_loss 0.2412\n",
      "[GPT][E2] step 1720/1842 | running_loss 0.2459\n",
      "[GPT][E2] step 1760/1842 | running_loss 0.2475\n",
      "[GPT][E2] step 1800/1842 | running_loss 0.2495\n",
      "[GPT][E2] step 1840/1842 | running_loss 0.2530\n",
      "[GPT][E3] step 40/1842 | running_loss 0.2462\n",
      "[GPT][E3] step 80/1842 | running_loss 0.2428\n",
      "[GPT][E3] step 120/1842 | running_loss 0.2448\n",
      "[GPT][E3] step 160/1842 | running_loss 0.2415\n",
      "[GPT][E3] step 200/1842 | running_loss 0.2453\n",
      "[GPT][E3] step 240/1842 | running_loss 0.2378\n",
      "[GPT][E3] step 280/1842 | running_loss 0.2398\n",
      "[GPT][E3] step 320/1842 | running_loss 0.2457\n",
      "[GPT][E3] step 360/1842 | running_loss 0.2390\n",
      "[GPT][E3] step 400/1842 | running_loss 0.2382\n",
      "[GPT][E3] step 440/1842 | running_loss 0.2524\n",
      "[GPT][E3] step 480/1842 | running_loss 0.2426\n",
      "[GPT][E3] step 520/1842 | running_loss 0.2397\n",
      "[GPT][E3] step 560/1842 | running_loss 0.2333\n",
      "[GPT][E3] step 600/1842 | running_loss 0.2417\n",
      "[GPT][E3] step 640/1842 | running_loss 0.2448\n",
      "[GPT][E3] step 680/1842 | running_loss 0.2383\n",
      "[GPT][E3] step 720/1842 | running_loss 0.2394\n",
      "[GPT][E3] step 760/1842 | running_loss 0.2441\n",
      "[GPT][E3] step 800/1842 | running_loss 0.2507\n",
      "[GPT][E3] step 840/1842 | running_loss 0.2300\n",
      "[GPT][E3] step 880/1842 | running_loss 0.2457\n",
      "[GPT][E3] step 920/1842 | running_loss 0.2341\n",
      "[GPT][E3] step 960/1842 | running_loss 0.2368\n",
      "[GPT][E3] step 1000/1842 | running_loss 0.2332\n",
      "[GPT][E3] step 1040/1842 | running_loss 0.2463\n",
      "[GPT][E3] step 1080/1842 | running_loss 0.2408\n",
      "[GPT][E3] step 1120/1842 | running_loss 0.2410\n",
      "[GPT][E3] step 1160/1842 | running_loss 0.2433\n",
      "[GPT][E3] step 1200/1842 | running_loss 0.2398\n",
      "[GPT][E3] step 1240/1842 | running_loss 0.2440\n",
      "[GPT][E3] step 1280/1842 | running_loss 0.2364\n",
      "[GPT][E3] step 1320/1842 | running_loss 0.2432\n",
      "[GPT][E3] step 1360/1842 | running_loss 0.2380\n",
      "[GPT][E3] step 1400/1842 | running_loss 0.2460\n",
      "[GPT][E3] step 1440/1842 | running_loss 0.2408\n",
      "[GPT][E3] step 1480/1842 | running_loss 0.2361\n",
      "[GPT][E3] step 1520/1842 | running_loss 0.2406\n",
      "[GPT][E3] step 1560/1842 | running_loss 0.2409\n",
      "[GPT][E3] step 1600/1842 | running_loss 0.2435\n",
      "[GPT][E3] step 1640/1842 | running_loss 0.2444\n",
      "[GPT][E3] step 1680/1842 | running_loss 0.2492\n",
      "[GPT][E3] step 1720/1842 | running_loss 0.2443\n",
      "[GPT][E3] step 1760/1842 | running_loss 0.2472\n",
      "[GPT][E3] step 1800/1842 | running_loss 0.2390\n",
      "[GPT][E3] step 1840/1842 | running_loss 0.2446\n"
     ]
    }
   ],
   "source": [
    "EPOCHS             = 3\n",
    "LR                 = 5e-5\n",
    "WEIGHT_DECAY       = 0.01\n",
    "GRAD_ACCUM_STEPS   = 8\n",
    "MAX_GRAD_NORM      = 1.0\n",
    "WARMUP_RATIO       = 0.06\n",
    "\n",
    "# Optimizer with weight decay on non-bias/LayerNorm\n",
    "decay, nodecay = [], []\n",
    "for n,p in gpt.named_parameters():\n",
    "    (decay if not any(nd in n for nd in [\"bias\",\"LayerNorm.weight\"]) else nodecay).append(p)\n",
    "gpt_optim = AdamW([{\"params\": decay, \"weight_decay\": WEIGHT_DECAY},\n",
    "                   {\"params\": nodecay, \"weight_decay\": 0.0}], lr=LR, fused=True)\n",
    "\n",
    "updates_per_epoch = math.ceil(len(gpt_train_loader) / GRAD_ACCUM_STEPS)\n",
    "total_updates     = EPOCHS * updates_per_epoch\n",
    "warmup_steps      = max(1, int(WARMUP_RATIO * total_updates))\n",
    "\n",
    "gpt_sched = get_linear_schedule_with_warmup(gpt_optim, warmup_steps, total_updates)\n",
    "scaler    = torch.amp.GradScaler(enabled=(device.type==\"cuda\"))\n",
    "\n",
    "def train_gpt():\n",
    "    global_step = 0\n",
    "    gpt.train()\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        running = 0.0\n",
    "        gpt_optim.zero_grad(set_to_none=True)\n",
    "        for step, batch in enumerate(gpt_train_loader, 1):\n",
    "            batch = {k: v.to(device, non_blocking=True) for k, v in batch.items()}\n",
    "            # PyTorch 2.x autocast\n",
    "            with torch.amp.autocast(device_type=device.type, enabled=(device.type==\"cuda\")):\n",
    "                out  = gpt(**batch)                 # labels mask the prompt to -100\n",
    "                loss = out.loss / GRAD_ACCUM_STEPS\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            running += loss.item()\n",
    "\n",
    "            if step % GRAD_ACCUM_STEPS == 0:\n",
    "                scaler.unscale_(gpt_optim)\n",
    "                clip_grad_norm_(gpt.parameters(), MAX_GRAD_NORM)\n",
    "                scaler.step(gpt_optim); scaler.update()\n",
    "                gpt_optim.zero_grad(set_to_none=True)\n",
    "                gpt_sched.step()\n",
    "                global_step += 1\n",
    "\n",
    "            LOG_EVERY = max(10, GRAD_ACCUM_STEPS * 5)\n",
    "\n",
    "            if step % LOG_EVERY == 0:\n",
    "                print(f\"[GPT][E{epoch}] step {step}/{len(gpt_train_loader)} | running_loss {running/LOG_EVERY:.4f}\")\n",
    "                running = 0.0\n",
    "\n",
    "\n",
    "train_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "27fcf96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVAL tokenizer (left padding)\n",
    "gpt_tok_eval = AutoTokenizer.from_pretrained(GPT_NAME, padding_side=\"left\")\n",
    "if gpt_tok_eval.pad_token is None:\n",
    "    gpt_tok_eval.pad_token = gpt_tok_eval.eos_token\n",
    "\n",
    "# Left-pad collator for decoder-only models\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List\n",
    "import torch\n",
    "\n",
    "@dataclass\n",
    "class GPTLeftPadCollator:\n",
    "    tokenizer: any\n",
    "    label_pad_id: int = -100\n",
    "    def __call__(self, features: List[Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:\n",
    "        pad_id = self.tokenizer.pad_token_id\n",
    "        bs = len(features)\n",
    "        max_len = max(f[\"input_ids\"].size(0) for f in features)\n",
    "\n",
    "        input_ids  = torch.full((bs, max_len), pad_id, dtype=torch.long)\n",
    "        attn_mask  = torch.zeros((bs, max_len), dtype=torch.long)\n",
    "        labels_out = torch.full((bs, max_len), self.label_pad_id, dtype=torch.long)\n",
    "\n",
    "        for i, f in enumerate(features):\n",
    "            L = f[\"input_ids\"].size(0)\n",
    "            input_ids[i,  -L:] = f[\"input_ids\"]\n",
    "            attn_mask[i,  -L:] = f[\"attention_mask\"]\n",
    "            labels_out[i,  -L:] = f[\"labels\"]\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attn_mask, \"labels\": labels_out}\n",
    "\n",
    "gpt_collator_eval = GPTLeftPadCollator(tokenizer=gpt_tok_eval)\n",
    "\n",
    "gpt_val_loader = DataLoader(\n",
    "    gpt_val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    collate_fn=gpt_collator_eval, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1676551a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_gpt(\n",
    "    model,\n",
    "    loader,\n",
    "    tokenizer,\n",
    "    max_new_tokens=96,\n",
    "    num_beams=1,\n",
    "    limit=None,\n",
    "    pad_to_multiple_of=None,     # None to disable; 8 helps tensor cores\n",
    "    min_new_tokens=8,\n",
    "    no_repeat_ngram_size=3,   # small repetition guard; set 0/None to disable\n",
    "    do_bertscore=False,        # turn off if you want max speed\n",
    "):\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    # Speed up AR decoding\n",
    "    was_ckpt = getattr(model, \"is_gradient_checkpointing\", False)\n",
    "    if was_ckpt:\n",
    "        model.gradient_checkpointing_disable()\n",
    "    if hasattr(model.config, \"use_cache\"):\n",
    "        model.config.use_cache = True\n",
    "\n",
    "    model.eval()\n",
    "    all_preds, all_refs = [], []\n",
    "    seen = 0\n",
    "\n",
    "    with torch.inference_mode(), torch.amp.autocast(device_type=device.type, enabled=(device.type==\"cuda\")):\n",
    "        for batch in loader:\n",
    "            if limit is not None and seen >= limit:\n",
    "                break\n",
    "\n",
    "            # ---- keep labels on CPU (we only need them to decode refs)\n",
    "            labs_cpu = batch[\"labels\"]  # (dont move to GPU)\n",
    "\n",
    "            # Move inputs to GPU once\n",
    "            ids  = batch[\"input_ids\"].to(device, non_blocking=True).long()\n",
    "            attn = batch[\"attention_mask\"].to(device, non_blocking=True).long()\n",
    "\n",
    "            # Sort by length to reduce padding/FLOPs (done on GPU)\n",
    "            lengths = attn.sum(dim=1)\n",
    "            order   = torch.argsort(lengths)          # GPU\n",
    "            inv     = torch.empty_like(order); inv[order] = torch.arange(order.numel(), device=device)\n",
    "\n",
    "            ids  = ids[order]\n",
    "            attn = attn[order]\n",
    "            lens_sorted = lengths[order]\n",
    "\n",
    "            # Optional: LEFT-pad to multiple of 8 (do it on GPU)\n",
    "            if pad_to_multiple_of:\n",
    "                pad_to = ((ids.size(1) + pad_to_multiple_of - 1) // pad_to_multiple_of) * pad_to_multiple_of\n",
    "                if pad_to > ids.size(1):\n",
    "                    left_pad = pad_to - ids.size(1)\n",
    "                    ids  = F.pad(ids,  (left_pad, 0), value=tokenizer.pad_token_id)\n",
    "                    attn = F.pad(attn, (left_pad, 0), value=0)\n",
    "\n",
    "            # Compute safe new-token budget for the whole sub-batch\n",
    "            max_ctx = getattr(tokenizer, \"model_max_length\", 1024) or 1024\n",
    "            longest_prompt = int(attn.sum(dim=1).max().item())\n",
    "            max_new = max(1, min(max_new_tokens, max_ctx - longest_prompt))\n",
    "\n",
    "            # ---- One batched generate() on GPU\n",
    "            gen = model.generate(\n",
    "                input_ids=ids,\n",
    "                attention_mask=attn,\n",
    "                max_new_tokens=max_new,\n",
    "                min_new_tokens=min_new_tokens,\n",
    "                num_beams=num_beams,          # 1 = greedy\n",
    "                do_sample=False,\n",
    "                no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "            )  # [B, prompt+gen] on GPU\n",
    "\n",
    "            # Slice completions by *padded input width* (padding-agnostic)\n",
    "            start = ids.size(1)                  # same for the whole sorted sub-batch\n",
    "            comps = gen[:, start:]               # still on GPU\n",
    "\n",
    "            # Decode preds (minimal CPU touch)\n",
    "            # Move only the completions we need to CPU for decoding\n",
    "            pred_texts_sorted = tokenizer.batch_decode(comps.cpu(), skip_special_tokens=True)\n",
    "\n",
    "            # Decode refs on CPU (labels never moved to GPU)\n",
    "            ref_texts_sorted = [\n",
    "                tokenizer.decode(l[l != -100], skip_special_tokens=True).strip()\n",
    "                for l in labs_cpu[order.cpu()]\n",
    "            ]\n",
    "\n",
    "            # Restore original order\n",
    "            inv_cpu = inv.cpu().tolist()\n",
    "            pred_texts = [pred_texts_sorted[k] for k in inv_cpu]\n",
    "            ref_texts  = [ref_texts_sorted[k]  for k in inv_cpu]\n",
    "\n",
    "            all_preds.extend(pred_texts)\n",
    "            all_refs.extend(ref_texts)\n",
    "            seen += len(pred_texts)\n",
    "\n",
    "            if limit is not None and seen >= limit:\n",
    "                break\n",
    "\n",
    "    # Restore training-time flags\n",
    "    if hasattr(model.config, \"use_cache\"):\n",
    "        model.config.use_cache = False\n",
    "    if was_ckpt:\n",
    "        model.gradient_checkpointing_enable()\n",
    "\n",
    "    # Filter empty pairs for metrics stability\n",
    "    pairs = [(p, r) for p, r in zip(all_preds, all_refs) if p and r]\n",
    "    if not pairs:\n",
    "        return all_preds, all_refs, {\"rouge1\":0, \"rouge2\":0, \"rougeL\":0}, 0.0\n",
    "    preds_f, refs_f = map(list, zip(*pairs))\n",
    "\n",
    "    # ROUGE (lightweight)  requires strings (CPU)\n",
    "    rouge = compute_rouge_light(preds_f, refs_f)\n",
    "\n",
    "    # (Optional) BERTScore  can be slow; toggle with do_bertscore\n",
    "    bert_f1 = 0.0\n",
    "    if do_bertscore:\n",
    "        try:\n",
    "            import evaluate\n",
    "            bertscore = evaluate.load(\"bertscore\")\n",
    "            bs = bertscore.compute(predictions=preds_f, references=refs_f, lang=\"en\")\n",
    "            bert_f1 = float(sum(bs[\"f1\"]) / len(bs[\"f1\"]))\n",
    "        except Exception:\n",
    "            bert_f1 = 0.0\n",
    "\n",
    "    return all_preds, all_refs, rouge, bert_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e517318e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.4268, 'rouge2': 0.4054, 'rougeL': 0.4255} | BERTScore F1: 0.0\n",
      "Saved: gpt_val_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# Run full eval (set limit to e.g. 200 to sample quickly)\n",
    "preds, refs, rouge_metrics, bert_f1 = evaluate_gpt(\n",
    "    gpt, gpt_val_loader, gpt_tok_eval, max_new_tokens=96, num_beams=1, limit=None\n",
    ")\n",
    "\n",
    "print({k: round(v, 4) for k, v in rouge_metrics.items()}, \"| BERTScore F1:\", round(bert_f1, 4))\n",
    "\n",
    "pd.DataFrame({\"pred\": preds, \"ref\": refs}).to_csv(\"gpt_val_predictions.csv\", index=False)\n",
    "print(\"Saved: gpt_val_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b5b5d869",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./checkpoints/distilgpt2-samsum\\\\tokenizer_config.json',\n",
       " './checkpoints/distilgpt2-samsum\\\\special_tokens_map.json',\n",
       " './checkpoints/distilgpt2-samsum\\\\vocab.json',\n",
       " './checkpoints/distilgpt2-samsum\\\\merges.txt',\n",
       " './checkpoints/distilgpt2-samsum\\\\added_tokens.json',\n",
       " './checkpoints/distilgpt2-samsum\\\\tokenizer.json')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt.save_pretrained(\"./checkpoints/distilgpt2-samsum\")\n",
    "gpt_tok.save_pretrained(\"./checkpoints/distilgpt2-samsum\")\n",
    "\n",
    "# Later:\n",
    "# gpt_tok = AutoTokenizer.from_pretrained(\"./checkpoints/distilgpt2-samsum\")\n",
    "# gpt     = AutoModelForCausalLM.from_pretrained(\"./checkpoints/distilgpt2-samsum\").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba5e784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- GPT samples ---\n",
      "\n",
      "[GPT] Example 1\n",
      "PRED:  Carol will book them.    Charles will pay a little more for the better seats.  Carol will let Carol know.  Charles is going to the Opera next month and will book the tickets.  He will book Carmen for the opera.  She will book her tickets. Carol is going on a trip to the theatre next month, so she will book a few more seats. Carol has to book the seats. She will be going to a theatre next year. \n",
      "REF : will book tickets for Carmen for himself and Carol.\n",
      "\n",
      "[GPT] Example 2\n",
      "PRED:  Jenny will let her in.    Jenny will get her flu.  Jenny has a key.  She will let herself in so she will take her key out.  Joins her to the shop in the morning.  The key is in the door.  It will be taken by Jenny.  He will let Jenny in. Joins Jenny in the afternoon.  They will have a cold and they will have flu. Join's key is taken by Jo\n",
      "REF : is coming down with a cold. Sue is doing grocery shopping for Jenny.\n",
      "\n",
      "[GPT] Example 3\n",
      "PRED:  Oliver is fed up.    Oliver is going to finish his paper.  Oliver finishes his paper in 10 seconds.  He finishes his papers in 10.  Liam is fed-up.  Oliver finishes his work in 10 sec.     \n",
      "Lionel: Liam is hungry. Oliver finishes the paper in ten seconds. Oliver finished his paper and is hungry, so he finishes his writing. \n",
      "Nelon:\n",
      "REF : and Oliver will meet in the hall in 10 minutes to have a break.\n",
      "\n",
      "[GPT] Example 4\n",
      "PRED:     Emma will plan another time.  Emma is planning another time, but will not go.  George will not be able to go again.  She will not have any more fun. Emma will not want to go back.  Emma will not come back. Emma is going back. She will be back.   Emmma will be home.  Emmia will be away. Emmia is going home. Em\n",
      "REF : was late and missed Andy's song, but she still had fun.\n",
      "\n",
      "[GPT] Example 5\n",
      "PRED:     Marc will let Rafael know about the gym on Monday.  Marc is going to let Rafael decide on Monday afternoon.  Rafael will let him know about his training on Monday morning.  Rafael will let Marc know about training on Tuesday afternoon. Marc will tell Rafael about training.  He will tell Rafael about training and Rafael's training. Marc is not going to go there.     Marc will tell Marc about training\n",
      "REF : overdid his training after a break yesterday and is too sore to go to the gym today. If fine by Tuesday, he will let Rafael know about it on Monday evening.\n"
     ]
    }
   ],
   "source": [
    "def show_samples(df, k=5, seed=13):\n",
    "    import numpy as np\n",
    "    np.random.seed(seed)\n",
    "    idx = np.random.choice(len(df), size=min(k, len(df)), replace=False)\n",
    "    for i, j in enumerate(idx, 1):\n",
    "        row = df.iloc[j]\n",
    "        print(f\"\\n[{row['model']}] Example {i}\")\n",
    "        print(\"PRED:\", row[\"pred\"])\n",
    "        print(\"REF :\", row[\"ref\"])\n",
    "gpt_df = pd.DataFrame({\"pred\": preds, \"ref\": refs})\n",
    "gpt_df[\"model\"] = \"GPT\"\n",
    "\n",
    "print(\"\\n--- GPT samples ---\")\n",
    "show_samples(gpt_df,  k=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
